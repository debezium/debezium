[id="debezium-platform"]
= {prodname} Management Platform

:linkattrs:
:icons: font
:toc:
:toclevels: 3
:toc-placement: macro

toc::[]

ifdef::community[]
[NOTE]
====
This project is currently in an incubating state.
The exact semantics, configuration options, and so forth are subject to change, based on the feedback that we receive.
====
endif::community[]

{prodname} Management Platform aims to simplify the deployment of {prodname} to various environments in a highly opinionated manner.
To achieve this goal, the platform uses a data-centric view of {prodname} components.

It is for us a normal evolution from {prodname} Server. We have provided the operator - to easily operate it in Kubernetes environments - and now we are providing a high-level abstraction to deploy your data pipelines in different environments leveraging on {prodname} Server.

== Basic concepts
In {prodname} Management Platform there are four main concepts:

Source:: Defines the source of your data
Destination:: Defines the destination of your data
Transform:: Define how a single data is transformed
Pipeline:: Defines how your data flows from a source to a destination while being transformed along the way.

Once you define a pipeline, how it will be deployed is in charge of the platform.

=== Architecture
The platform is composed of two main components:

Conductor:: The back-end component which provides a set of APIs to orchestrate and control {prodname} deployments.
Stage:: The front-end component which provides a user interface to interact with the Conductor.

The conductor component itself is composed of several subcomponents:

API Server:: The main entry point, it provides a set of APIs to interact with the platform.
Watcher:: Component responsible for the actual communication with deployment environment (e.g. {prodname} Operator in K8s cluster).

image::debezium-platform-architecture.svg[{prodname} Platform Architecture]

== Installation

[NOTE]
====
Currently, the only supported environment is Kubernetes.
====

**Prerequisites**

* Helm
* Kubernetes cluster with an ingress controller

Installation is provided trough https://helm.sh/[Helm] chart.
The first step is to add the {prodname} charts repos with the following command:

[source,bash]
----
helm repo add debezium https://charts.debezium.io
----

after that, you can install a specific version with the following command:

[source, bash]
----
helm install debezium-platform debezium/debezium-platform --version 3.1.0-beta1 --set database.enabled=true --set domain.url=platform.debezium.io
----

as an alternative to this, you can also install it via OCI artifact with the following command:

[source, bash]
----
helm install debezium-platform --set database.enabled=true --set domain.url=platform.debezium.io --version 3.1.0-beta1 oci://quay.io/debezium-charts/debezium-platform
----

the `domain.url` is the only required property, and it is used as `host` in the `Ingress` definition.
The `database.enabled` properties has been added for simplicity since in that way the PostgreSQL database, required by the conductor service, is automatically deployed.
This is discouraged in production environment, so in that case please do not use it and provide instead your database instance.

The following tables lists all the chart's properties:

[cols="1,3,1", options="header"]
|===
|Name |Description |Default

|domain.url
|domain used as ingress host
|""

|stage.image
|Image for the stage (UI)
|quay.io/debezium/platform-stage:<release_tag>

|conductor.image
|Image for the conductor
|quay.io/debezium/platform-conductor:<release_tag>

|conductor.offset.existingConfigMap
|Name of the config map used to store conductor offsets. If empty it will be automatically created.
|""

|database.enabled
|Enable the installation of PostgreSQL by the chart
|false

|database.name
|Name of the database used by the platform
|postgres

|database.host
|Host of the database used by the platform
|postgres

|database.auth.existingSecret
|Name of the secret where `username` and `password` - of the database used by the platform - are stored . If empty a secret will be created using the `username` and `password` properties.

When this is used, you don't need to set `database.auth.username` and `database.auth.password`.
|""

|database.auth.username
|Username of the database used by the platform
|user

|database.auth.password
|Password of the database used by the platform
|password

|offset.reusePlatformDatabase
|Pipelines will use database to store offsets. By default, the database used by the platform is used.
If you want to use a dedicated one set this property to false
|true

|offset.database.name
|Name of the database used by the platform for storing the offsets
|postgres

|offset.database.host
|Host of the database used by the platform for storing the offsets
|postgres

|offset.database.port
|Port of the database used by the platform for storing the offsets
|5432

|offset.database.auth.existingSecret
|Name of the secret where `username` and `password` - of the database used by the platform for storing the offsets - are stored. If not set `offset.database.auth.username` and `offset.database.auth.password` will be used.

When this is used, you don't need to set `offset.database.auth.username` and `offset.database.auth.password`.
|""

|offset.database.auth.username
|Username of the database used by the platform for storing the offsets
|user

|offset.database.auth.password
|Password of the database used by the platform for storing the offsets
|password

|schemaHistory.reusePlatformDatabase
|Pipelines will use database to store schema history. By default, the database used by the conductor service is used. If you want to use a dedicated one set this property to false
|true

|schemaHistory.database.name
|Name of the database used by the platform for storing the schema history
|postgres

|schemaHistory.database.host
|Host of the database used by the platform for storing the schema history
|postgres

|schemaHistory.database.port
|Port of the database used by the platform for storing the schema history
|5432

|schemaHistory.database.auth.existingSecret
|Name of the secret where `username` and `password` - of the database used by the platform for storing the schema history -are stored. If not set `schemaHistory.database.auth.username` and `schemaHistory.database.auth.password` will be used.

When this is used, you don't need to set `schemaHistory.database.auth.username` and `schemaHistory.database.auth.password`.
|""

|schemaHistory.database.auth.username
|Username of the database used by the platform for storing the schema history
|user

|schemaHistory.database.auth.password
|Password of the database used by the platform for storing the schema history
|password

|env
|List of env variable to pass to the conductor
|[]
|===

== Using the platform

In this section we will do a walkthrough of the different functionalities of the UI.

=== Source
In this section, you can define the sources of your data.
All {prodname} supported databases are available.
When you create a source, it can be shared between different pipelines, which means that every change to a source will be reflected in every pipeline that uses it.

==== Create a new source
In this section, you can configure your source in two different ways. You can use the `Form Editor`, where you can enter the name of the source and a description, and then specify the list of properties for the specific source.
Refer to the connector-specific documentation page for the available properties.

==== Configure the source with Smart Editor
The other option is the `Smart Editor`, where you can directly edit the `JSON` configuration.
For those familiar with {prodname}, this is quite similar to the Kafka Connect configuration or {prodname} Server with small differences.
The common part is the `config` section, in fact you can more or less copy the standard {prodname} configuration `config` section under the `config` property.

For example, if you have the following configuration:

[source,json,options="nowrap"]
----
{
  "name": "inventory-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "mysql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "dbz",
    "database.server.id": "184054",
    "topic.prefix": "dbserver1",
    "database.include.list": "inventory"
  }
}
----

You just need to copy the `config` section, removing the `connector.class`, since it is already provided with the `type`.
In the future we will eventually support the Kafka Connect and/or Debezium Server format directly.

[source,json,options="nowrap"]
----
{
    "name": "my-source",
    "description": "This is my first source",
    "type": "io.debezium.connector.mysql.MySqlConnector",
    "schema": "schema123",
    "vaults": [],
    "config": {
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
        "database.password": "dbz",
        "database.server.id": "184054",
        "topic.prefix": "dbserver1",
        "database.include.list": "inventory"
    }
}
----

==== Delete a source
To delete a source, go to the `Source` menu and then click the `action` menu of the source you want to delete, then click `Delete`.
A source can be deleted only if it is not used in any pipeline; otherwise, you will receive an error.
When the source is no longer used in any pipeline, you can delete it using the `Delete` option.

==== Edit a source
To edit a source, go to the `Source` menu and then click the `action` menu of the source you want to edit, then click `Edit`.

[NOTE]
====
Editing a source will affect all pipelines that use it.
====

=== Destination
In this section, you can define the destinations where your source data will be sent.
All {prodname} supported databases and systems are available.
When you create a destination, it can be shared between different pipelines, which means that every change to a destination will be reflected in every pipeline that uses it.

==== Create a new destination
In this section, you can configure your destination in two different ways. You can use the Form Editor, where you can enter the name of the destination and a description, and then specify the list of properties for the specific destination system.
Refer to the {prodname} sink-specific documentation page for the available properties.

==== Configure the destination with Smart Editor
The other option is the Smart Editor, where you can directly edit the JSON configuration.
For those familiar with {prodname}, this is quite similar to {prodname} Server `sink` configuration section with small differences.
Usually you have that the configuration of a particular sink are prefixed with `debezium.sink.<sink_name>` where `<sink_name` is the sink `type`.

For example, if you have the following configuration:

[source,properties,options="nowrap"]
----
# ...

debezium.sink.type=pubsub
debezium.sink.pubsub.project.id=debezium-tutorial-local
debezium.sink.pubsub.address=pubsub:8085

# ..
----

You just need to take all properties prefixed with `debezium.sink.pubsub` and transform in `json` format.
In the future we will eventually support the Kafka Connect and/or Debezium Server format directly.

[source,json,options="nowrap"]
----
{
  "name": "test-destination",
  "type": "pubsub",
  "description": "Some funny destination",
  "schema": "dummy",
  "vaults": [],
  "config": {
    "project.id": "debezium-tutorial-local",
    "address": "pubsub:8085"
  }
}
----

==== Delete a destination
To delete a destination, go to the `Destination` menu and then click the `action` menu of the destination you want to delete, then click `Delete`.
A destination can be deleted only if it is not used in any pipeline; otherwise, you will receive an error.
When the destination is no longer used in any pipeline, you can delete it using the `Delete` button.

==== Edit a destination
To edit a destination, go to the `Destination` menu and then click the `action` menu of the destination you want to edit, then click `Edit`.

[NOTE]
Editing a destination will affect all pipelines that use it.

=== Transforms
==== Create
==== Edit
==== Delete
=== Pipeline
==== Create
==== Delete
==== Edit
===== Transformations
=== Monitoring

