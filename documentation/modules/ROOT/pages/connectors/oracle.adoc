[id="debezium-connector-for-oracle"]
= {prodname} Connector for Oracle

:context: oracle
:toc:
:toc-placement: macro
:linkattrs:
:icons: font

toc::[]

[[oracle-overview]]
== Overview

{prodname} ingests change events from Oracle using the native LogMiner database package or the https://docs.oracle.com/database/121/XSTRM/xstrm_intro.htm#XSTRM72647[XStream API].
While the connector may work with a variety of Oracle versions and editions, only Oracle EE 12 and 19 have been tested.

[[how-the-oracle-connector-works]]
== How the Oracle Connector Works

To optimally configure and run a {prodname} Oracle connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata.

[[oracle-snapshots]]
=== Snapshots

Most Oracle servers are configured to not retain the complete history of the database in the redo logs,
so the {prodname} Oracle connector would be unable to see the entire history of the database by simply reading the logs.
Consequently, the first time the connector starts, it performs an initial _consistent snapshot_ of the database.
the default behavior for performing a snapshot consists of the following steps.
you can change this behavior by setting the `snapshot.mode` connector configuration property to a value other than `initial`.

1. Determine the tables to be captured
2. Obtain a `ROW SHARE MODE` lock on each of the monitored tables to ensure that no structural changes can occur to any of the tables.
3. Read the current SCN ("system change number") position in the server's redo log.
4. Capture the structure of all relevant tables.
5. Release the locks obtained in step 2, i.e. the locks are held only for a short period of time.
6. Scan all of the relevant database tables and schemas as valid at the SCN position read in step 3 (`SELECT * FROM ... AS OF SCN 123`), and generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.
7. Record the successful completion of the snapshot in the connector offsets.

If the connector fails, is rebalanced, or stops after step 1 begins but before step 7 completes,
upon restart the connector will begin a new snapshot.
After the connector completes its initial snapshot, the {prodname} connector continues streaming from the position that it read in step 3.
This ensures that the connector does not miss any updates.
If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.

.Settings for `snapshot.mode` connector configuration property
[cols="30%a,70%a",options="header"]
|===
|Setting |Description

|`initial`
|The connector performs a database snapshot after which it will transition to streaming changes.

|`schema_only`
|The connector captures the structure of all relevant tables, performing all the steps described above, except it does not create any `READ` events representing the dataset at the point of the connector's start-up.

|===

[[oracle-topic-names]]
=== Topics Names

[[oracle-schema-change-topic]]
=== Schema Change Topic

The {prodname} Oracle connector stores the history of schema changes in a database history topic.
This topic reflects an internal connector state and you should not use it directly.
Applications that require notifications about schema changes should obtain the information from the public schema change topic.
the connector writes all of these events to a Kafka topic named `<serverName>`, where `serverName` is the name of the connector that is specified in the `database.server.name` configuration property.

[WARNING]
====
The schema change topic message format is in an incubating state and may change without notice.
====

{prodname} emits a new message to this topic whenever a new table is streamed from or when the structure of the table is altered.
The message contains a logical representation of the table schema.

The example of the message is:

[source,json,indent=0,subs="+attributes"]
----
{
  "schema": {
  ...
  },
  "payload": {
    "source": {
      "version": "{debezium-version}",
      "connector": "oracle",
      "name": "server1",
      "ts_ms": 1588252618953,
      "snapshot": "true",
      "db": "ORCLPDB1",
      "schema": "DEBEZIUM",
      "table": "CUSTOMERS",
      "txId" : null,
      "scn" : "1513734",
      "commit_scn": "1513734",
      "lcr_position" : null
    },
    "databaseName": "ORCLPDB1", // <1>
    "schemaName": "DEBEZIUM", // <1>
    "ddl": "CREATE TABLE \"DEBEZIUM\".\"CUSTOMERS\" \n   (    \"ID\" NUMBER(9,0) NOT NULL ENABLE, \n    \"FIRST_NAME\" VARCHAR2(255), \n    \"LAST_NAME" VARCHAR2(255), \n    \"EMAIL\" VARCHAR2(255), \n     PRIMARY KEY (\"ID\") ENABLE, \n     SUPPLEMENTAL LOG DATA (ALL) COLUMNS\n   ) SEGMENT CREATION IMMEDIATE \n  PCTFREE 10 PCTUSED 40 INITRANS 1 MAXTRANS 255 \n NOCOMPRESS LOGGING\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1\n  BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\n  TABLESPACE \"USERS\" ", // <2>
    "tableChanges": [ // <3>
      {
        "type": "CREATE", // <4>
        "id": "\"ORCLPDB1\".\"DEBEZIUM\".\"CUSTOMERS\"", // <5>
        "table": { // <6>
          "defaultCharsetName": null,
          "primaryKeyColumnNames": [ // <7>
            "ID"
          ],
          "columns": [ // <8>
            {
              "name": "ID",
              "jdbcType": 2,
              "nativeType": null,
              "typeName": "NUMBER",
              "typeExpression": "NUMBER",
              "charsetName": null,
              "length": 9,
              "scale": 0,
              "position": 1,
              "optional": false,
              "autoIncremented": false,
              "generated": false
            },
            {
              "name": "FIRST_NAME",
              "jdbcType": 12,
              "nativeType": null,
              "typeName": "VARCHAR2",
              "typeExpression": "VARCHAR2",
              "charsetName": null,
              "length": 255,
              "scale": null,
              "position": 2,
              "optional": false,
              "autoIncremented": false,
              "generated": false
            },
            {
              "name": "LAST_NAME",
              "jdbcType": 12,
              "nativeType": null,
              "typeName": "VARCHAR2",
              "typeExpression": "VARCHAR2",
              "charsetName": null,
              "length": 255,
              "scale": null,
              "position": 3,
              "optional": false,
              "autoIncremented": false,
              "generated": false
            },
            {
              "name": "EMAIL",
              "jdbcType": 12,
              "nativeType": null,
              "typeName": "VARCHAR2",
              "typeExpression": "VARCHAR2",
              "charsetName": null,
              "length": 255,
              "scale": null,
              "position": 4,
              "optional": false,
              "autoIncremented": false,
              "generated": false
            }
          ]
        }
      }
    ]
  }
}
----

.Descriptions of fields in messages emitted to the schema change topic
[cols="1,4,5",options="header"]
|===
|Item |Field name |Description

|1
|`databaseName` +
`schemaName`
|Identifies the database and the schema that contain the change.

|2
|`ddl`
|This field contains the DDL responsible for the schema change.

|3
|`tableChanges`
|An array of one or more items that contain the schema changes generated by a DDL command.

|4
|`type`
a|Describes the kind of change. The value is one of the following:

* `CREATE` - table created
* `ALTER` - table modified
* `DROP` - table deleted

|5
|`id`
|Full identifier of the table that was created, altered, or dropped.
In the case of a table rename, this will be a concatenation of `<old>,<new>` table names.

|6
|`table`
|Represents table metadata after the applied change.

|7
|`primaryKeyColumnNames`
|List of columns that compose the table's primary key.

|8
|`columns`
|Metadata for each column in the changed table.

|===

In messages that the connector sends to the schema change topic, the key is the name of the database that contains the schema change.
In the following example, the `payload` field contains the key:

[source,json,indent=0,subs="+attributes"]
----
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "string",
        "optional": false,
        "field": "databaseName"
      }
    ],
    "optional": false,
    "name": "io.debezium.connector.oracle.SchemaChangeKey"
  },
  "payload": {
    "databaseName": "ORCLPDB1"
  }
}
----

[[oracle-transaction-metadata]]
=== Transaction Metadata

{prodname} can generate events that represents transaction metadata boundaries and enrich data messages.

==== Transaction boundaries
{prodname} generates events for every transaction `BEGIN` and `END`.
Every event contains

* `status` - `BEGIN` or `END`
* `id` - string representation of unique transaction identifier
* `event_count` (for `END` events) - total number of events emmitted by the transaction
* `data_collections` (for `END` events) - an array of pairs of `data_collection` and `event_count` that provides number of events emitted by changes originating from given data collection

Following is an example of what a message looks like:

[source,json,indent=0,subs="attributes"]
----
{
  "status": "BEGIN",
  "id": "5.6.641",
  "event_count": null,
  "data_collections": null
}

{
  "status": "END",
  "id": "5.6.641",
  "event_count": 2,
  "data_collections": [
    {
      "data_collection": "ORCLPDB1.DEBEZIUM.CUSTOMER",
      "event_count": 1
    },
    {
      "data_collection": "ORCLPDB1.DEBEZIUM.ORDER",
      "event_count": 1
    }
  ]
}
----

The transaction events are written to the topic named `<database.server.name>.transaction`.

==== Data events enrichment
When transaction metadata is enabled the data message `Envelope` is enriched with a new `transaction` field.
This field provides information about every event in the form of a composite of fields:

* `id` - string representation of unique transaction identifier
* `total_order` - the absolute position of the event among all events generated by the transaction
* `data_collection_order` - the per-data collection position of the event among all events that were emitted by the transaction

Following is an example of what a message looks like:

[source,json,indent=0,subs="attributes"]
----
{
  "before": null,
  "after": {
    "pk": "2",
    "aa": "1"
  },
  "source": {
...
  },
  "op": "c",
  "ts_ms": "1580390884335",
  "transaction": {
    "id": "5.6.641",
    "total_order": "1",
    "data_collection_order": "1"
  }
}
----

[[oracle-events]]
== Data change events

All data change events produced by the Oracle connector have a key and a value, although the structure of the key and value depend on the table from which the change events originated (see {link-prefix}:{link-oracle-connector}#oracle-topic-names[Topic names]).

[WARNING]
====
The {prodname} Oracle connector ensures that all Kafka Connect _schema names_ are http://avro.apache.org/docs/current/spec.html#names[valid Avro schema names].
This means that the logical server name must start with Latin letters or an underscore (e.g., [a-z,A-Z,\_]),
and the remaining characters in the logical server name and all characters in the schema and table names must be Latin letters, digits, or an underscore (e.g., [a-z,A-Z,0-9,\_]).
If not, then all invalid characters will automatically be replaced with an underscore character.

This can lead to unexpected conflicts when the logical server name, schema names, and table names contain other characters, and the only distinguishing characters between table full names are invalid and thus replaced with underscores.
====

{prodname} and Kafka Connect are designed around _continuous streams of event messages_, and the structure of these events may change over time.
This could be difficult for consumers to deal with, so to make it easy Kafka Connect makes each event self-contained.
Every message key and value has two parts: a _schema_ and _payload_.
The schema describes the structure of the payload, while the payload contains the actual data.

[[oracle-change-event-keys]]
=== Change event keys

For a given table, the change event's key will have a structure that contains a field for each column in the primary key (or unique key constraint) of the table at the time the event was created.

Consider a `customers` table defined in the `inventory` database schema:

[source,sql,indent=0]
----
CREATE TABLE customers (
  id NUMBER(9) GENERATED BY DEFAULT ON NULL AS IDENTITY (START WITH 1001) NOT NULL PRIMARY KEY,
  first_name VARCHAR2(255) NOT NULL,
  last_name VARCHAR2(255) NOT NULL,
  email VARCHAR2(255) NOT NULL UNIQUE
);
----

If the `database.server.name` configuration property has the value `server1`,
every change event for the `customers` table while it has this definition will feature the same key structure, which in JSON looks like this:

[source,json,indent=0,sub="attributes"]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "int32",
                "optional": false,
                "field": "ID"
            }
        ],
        "optional": false,
        "name": "server1.INVENTORY.CUSTOMERS.Key"
    },
    "payload": {
        "ID": 1004
    }
}
----

The `schema` portion of the key contains a Kafka Connect schema describing what is in the key portion, and in our case that means that the `payload` value is not optional, is a structure defined by a schema named `server1.DEBEZIUM.CUSTOMERS.Key`, and has one required field named `id` of type `int32`.
If you look at the value of the key's `payload` field, you can see that it is indeed a structure (which in JSON is just an object) with a single `id` field, whose value is `1004`.

Therefore, you can interpret this key as describing the row in the `inventory.customers` table (output from the connector named `server1`) whose `id` primary key column had a value of `1004`.

////
[NOTE]
====
Although the `column.exclude.list` configuration property allows you to remove columns from the event values, all columns in a primary or unique key are always included in the event's key.
====

[WARNING]
====
If the table does not have a primary or unique key, then the change event's key will be null. This makes sense since the rows in a table without a primary or unique key constraint cannot be uniquely identified.
====
////

[[oracle-change-event-values]]
=== Change event values

Like the message key, the value of a change event message has a _schema_ section and _payload_ section.
The payload section of every change event value produced by the Oracle connector has an _envelope_ structure with the following fields:

* `op` is a mandatory field that contains a string value describing the type of operation. Values for the Oracle connector are `c` for create (or insert), `u` for update, `d` for delete, and `r` for read (in the case of a snapshot).
* `before` is an optional field that if present contains the state of the row _before_ the event occurred. The structure will be described by the `server1.INVENTORY.CUSTOMERS.Value` Kafka Connect schema, which the `server1` connector uses for all rows in the `inventory.customers` table.

[WARNING]
====
Whether or not this field and its elements are available is highly dependent on the https://docs.oracle.com/database/121/SUTIL/GUID-D2DDD67C-E1CC-45A6-A2A7-198E4C142FA3.htm#SUTIL1583[Supplemental Logging] configuration applying to the table.
====

* `after` is an optional field that if present contains the state of the row _after_ the event occurred. The structure is described by the same `server1.INVENTORY.CUSTOMERS.Value` Kafka Connect schema used in `before`.
* `source` is a mandatory field that contains a structure describing the source metadata for the event, which in the case of Oracle contains these fields: the {prodname} version, the connector name, whether the event is part of an ongoing snapshot or not, the transaction id (not while snapshotting), the SCN of the change, and a timestamp representing the point in time when the record was changed in the source database (during snapshotting, this is the point in time of snapshotting).

[TIP]
====
The `commit_scn` field is optional and describes the SCN of the transaction commit that the change event participates within.
This field is only present when using the LogMiner connection adapter.
====

* `ts_ms` is optional and if present contains the time (using the system clock in the JVM running the Kafka Connect task) at which the connector processed the event.

And of course, the _schema_ portion of the event message's value contains a schema that describes this envelope structure and the nested fields within it.

[[oracle-create-events]]
=== Create events

Let's look at what a _create_ event value might look like for our `customers` table:

[source,json,indent=0,subs="+attributes"]
----
{
    "schema": {
        "type": "struct",
        "fields": [
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "ID"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "FIRST_NAME"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "LAST_NAME"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "EMAIL"
                    }
                ],
                "optional": true,
                "name": "server1.DEBEZIUM.CUSTOMERS.Value",
                "field": "before"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "int32",
                        "optional": false,
                        "field": "ID"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "FIRST_NAME"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "LAST_NAME"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "EMAIL"
                    }
                ],
                "optional": true,
                "name": "server1.DEBEZIUM.CUSTOMERS.Value",
                "field": "after"
            },
            {
                "type": "struct",
                "fields": [
                    {
                        "type": "string",
                        "optional": true,
                        "field": "version"
                    },
                    {
                        "type": "string",
                        "optional": false,
                        "field": "name"
                    },
                    {
                        "type": "int64",
                        "optional": true,
                        "field": "ts_ms"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "txId"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "scn"
                    },
                    {
                        "type": "string",
                        "optional": true,
                        "field": "commit_scn"
                    },
                    {
                        "type": "boolean",
                        "optional": true,
                        "field": "snapshot"
                    }
                ],
                "optional": false,
                "name": "io.debezium.connector.oracle.Source",
                "field": "source"
            },
            {
                "type": "string",
                "optional": false,
                "field": "op"
            },
            {
                "type": "int64",
                "optional": true,
                "field": "ts_ms"
            }
        ],
        "optional": false,
        "name": "server1.DEBEZIUM.CUSTOMERS.Envelope"
    },
    "payload": {
        "before": null,
        "after": {
            "ID": 1004,
            "FIRST_NAME": "Anne",
            "LAST_NAME": "Kretchmar",
            "EMAIL": "annek@noanswer.org"
        },
        "source": {
            "version": "{debezium-version}",
            "name": "server1",
            "ts_ms": 1520085154000,
            "txId": "6.28.807",
            "scn": "2122185",
            "commit_scn": "2122185",
            "snapshot": false
        },
        "op": "c",
        "ts_ms": 1532592105975
    }
}
----

If we look at the `schema` portion of this event's _value_, we can see the schema for the _envelope_, the schema for the `source` structure (which is specific to the Oracle connector and reused across all events), and the table-specific schemas for the `before` and `after` fields.

[TIP]
====
The names of the schemas for the `before` and `after` fields are of the form _logicalName_._schemaName_._tableName_.Value, and thus are entirely independent from all other schemas for all other tables.
This means that when using the link:/docs/faq/#avro-converter[Avro Converter], the resulting Avro schems for _each table_ in each _logical source_ have their own evolution and history.
====

If we look at the `payload` portion of this event's _value_, we can see the information in the event, namely that it is describing that the row was created (since `op=c`), and that the `after` field value contains the values of the new inserted row's' `ID`, `FIRST_NAME`, `LAST_NAME`, and `EMAIL` columns.

[TIP]
====
It may appear that the JSON representations of the events are much larger than the rows they describe.
This is true, because the JSON representation must include the _schema_ and the _payload_ portions of the message.
It is possible and even recommended to use the link:/docs/faq/#avro-converter[Avro Converter] to dramatically decrease the size of the actual messages written to the Kafka topics.
====

[[oracle-update-events]]
=== Update events
The value of an _update_ change event on this table will actually have the exact same _schema_, and its payload will be structured the same but will hold different values.
Here's an example:

[source,json,indent=0,subs="+attributes"]
----
{
    "schema": { ... },
    "payload": {
        "before": {
            "ID": 1004,
            "FIRST_NAME": "Anne",
            "LAST_NAME": "Kretchmar",
            "EMAIL": "annek@noanswer.org"
        },
        "after": {
            "ID": 1004,
            "FIRST_NAME": "Anne",
            "LAST_NAME": "Kretchmar",
            "EMAIL": "anne@example.com"
        },
        "source": {
            "version": "{debezium-version}",
            "name": "server1",
            "ts_ms": 1520085811000,
            "txId": "6.9.809",
            "scn": "2125544",
            "commit_scn": "2125544",
            "snapshot": false
        },
        "op": "u",
        "ts_ms": 1532592713485
    }
}
----

When we compare this to the value in the _insert_ event, we see a couple of differences in the `payload` section:

* The `op` field value is now `u`, signifying that this row changed because of an update
* The `before` field now has the state of the row with the values before the database commit
* The `after` field now has the updated state of the row, and here was can see that the `EMAIL` value is now `anne@example.com`.
* The `source` field structure has the same fields as before, but the values are different since this event is from a different position in the redo log.
* The `ts_ms` shows the timestamp that {prodname} processed this event.

There are several things we can learn by just looking at this `payload` section. We can compare the `before` and `after` structures to determine what actually changed in this row because of the commit.
The `source` structure tells us information about Oracle's record of this change (providing traceability), but more importantly this has information we can compare to other events in this and other topics to know whether this event occurred before, after, or as part of the same Oracle commit as other events.

[NOTE]
====
When the columns for a row's primary/unique key are updated, the value of the row's key has changed so {prodname} will output _three_ events: a `DELETE` event and a {link-prefix}:{link-oracle-connector}#oracle-tombstone-events[tombstone event] with the old key for the row, followed by an `INSERT` event with the new key for the row.
====

[[oracle-delete-events]]
=== Delete events

So far we've seen samples of _create_ and _update_ events.
Now, let's look at the value of a _delete_ event for the same table. Once again, the `schema` portion of the value will be exactly the same as with the _create_ and _update_ events:

[source,json,indent=0,subs="+attributes"]
----
{
    "schema": { ... },
    "payload": {
        "before": {
            "ID": 1004,
            "FIRST_NAME": "Anne",
            "LAST_NAME": "Kretchmar",
            "EMAIL": "anne@example.com"
        },
        "after": null,
        "source": {
            "version": "{debezium-version}",
            "name": "server1",
            "ts_ms": 1520085153000,
            "txId": "6.28.807",
            "scn": "2122184",
            "commit_scn": "2122184",
            "snapshot": false
        },
        "op": "d",
        "ts_ms": 1532592105960
    }
}
----

If we look at the `payload` portion, we see a number of differences compared with the _create_ or _update_ event payloads:

* The `op` field value is now `d`, signifying that this row was deleted
* The `before` field now has the state of the row that was deleted with the database commit.
* The `after` field is null, signifying that the row no longer exists
* The `source` field structure has many of the same values as before, except the `ts_ms`, `scn` and `txId` fields have changed
* The `ts_ms` shows the timestamp that {prodname} processed this event.

This event gives a consumer all kinds of information that it can use to process the removal of this row.

The Oracle connector's events are designed to work with https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction[Kafka log compaction],
which allows for the removal of some older messages as long as at least the most recent message for every key is kept.
This allows Kafka to reclaim storage space while ensuring the topic contains a complete dataset and can be used for reloading key-based state.

[[oracle-tombstone-events]]
When a row is deleted, the _delete_ event value listed above still works with log compaction, since Kafka can still remove all earlier messages with that same key.
But only if the message value is `null` will Kafka know that it can remove _all messages_ with that same key.
To make this possible, {prodname}'s Oracle connector always follows the _delete_ event with a special _tombstone_ event that has the same key but `null` value.

[[oracle-data-type-mappings]]
== Data Type mappings

The Oracle connector represents changes to rows with events that are structured like the table in which the rows exists. The event contains a field for each column value. How that value is represented in the event depends on the Oracle data type of the column. The following sections describe how the connector maps oracle data types to a _literal type_ and a _semantic type_ in event fields.

* _literal type_ describes how the value is literally represented using Kafka Connect schema types: `INT8`, `INT16`, `INT32`, `INT64`, `FLOAT32`, `FLOAT64`, `BOOLEAN`, `STRING`, `BYTES`, `ARRAY`, `MAP`, and `STRUCT`.

* _semantic type_ describes how the Kafka Connect schema captures the _meaning_ of the field using the name of the Kafka Connect schema for the field.

ifdef::product[]
Details are in the following sections:

* xref:oracle-character-types[]
* xref:oracle-numeric-types[]
* xref:oracle-decimal-types[]
* xref:oracle-temporal-types[]

endif::product[]

ifdef::community[]
Support for further data types will be added in subsequent releases.
Please file a {jira-url}/browse/DBZ[JIRA issue] for any specific types that may be missing.
endif::community[]

[id="oracle-character-types"]
=== Character and BLOB types

The following table describes how the connector maps character and blob types.

.Mappings for Oracle character and blob data types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`BLOB`
|`BYTES`
|The raw bytes.

|`CHAR[(M)]`
|`STRING`
|n/a

|`CLOB`
|`STRING`
|n/a

|`LONG`
|n/a
|_This data type is not supported._

|`LONG RAW`
|n/a
|_This data type is not supported._

|`NCHAR[(M)]`
|`STRING`
|n/a

|`NCLOB`
|`STRING`
|n/a

|`NVARCHAR2[(M)]`
|`STRING`
|n/a

|`RAW`
|n/a
|_This data type is not supported._

|`VARCHAR[(M)]`
|`STRING`
|n/a

|`VARCHAR2[(M)]`
|`STRING`
|n/a

|===

[id="oracle-numeric-types"]
=== Numeric types

The following table describes how the connector maps numeric types.

.Mappings for Oracle numeric data types
[cols="20%a,15%a,55%a",options="header"]
|===
|Oracle Data Type
|Literal type (schema type)
|Semantic type (schema name) and Notes

|`BINARY_FLOAT`
|`FLOAT32`
|n/a

|`BINARY_DOUBLE`
|`FLOAT64`
|n/a

|`DECIMAL[(P, S)]`
|`BYTES` / `INT8` / `INT16` / `INT32` / `INT64`
|`org.apache.kafka.connect.data.Decimal` if using `BYTES` +
 +
Handled equivalently to `NUMBER` (note that S defaults to 0 for `DECIMAL`).

|`DOUBLE PRECISION`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|`FLOAT[(P)]`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|`INTEGER`, `INT`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal` +
 +
`INTEGER` is mapped in Oracle to NUMBER(38,0) and hence can hold values larger than any of the `INT` types could store

|`NUMBER[(P[, *])]`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|`NUMBER(P, S \<= 0)`
|`INT8` / `INT16` / `INT32` / `INT64`
|`NUMBER` columns with a scale of 0 represent integer numbers; a negative scale indicates rounding in Oracle, e.g. a scale of -2 will cause rounding to hundreds. +
 +
Depending on the precision and scale, a matching Kafka Connect integer type will be chosen: +

 * P - S < 3, `INT8` +
 * P - S < 5, `INT16` +
 * P - S < 10, `INT32` +
 * P - S < 19, `INT64` +
 * P - S >= 19, `BYTES` (`org.apache.kafka.connect.data.Decimal`).

|`NUMBER(P, S > 0)`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal`

|`NUMERIC[(P, S)]`
|`BYTES` / `INT8` / `INT16` / `INT32` / `INT64`
|`org.apache.kafka.connect.data.Decimal` if using `BYTES` +
 +
Handled equivalently to `NUMBER` (note that S defaults to 0 for `NUMERIC`).

|`SMALLINT`
|`BYTES`
|`org.apache.kafka.connect.data.Decimal` +
 +
`SMALLINT` is mapped in Oracle to NUMBER(38,0) and hence can hold values larger than any of the `INT` types could store

|`REAL`
|`STRUCT`
|`io.debezium.data.VariableScaleDecimal` +
 +
Contains a structure with two fields: `scale` of type `INT32` that contains the scale of the transferred value and `value` of type `BYTES` containing the original value in an unscaled form.

|===

[id="oracle-boolean-types"]
=== Boolean types

Oracle does not natively have support for a `BOOLEAN` data type; however,
it is common practice to use other data types with certain semantics to simulate the concept of a logical `BOOLEAN` data type.

The operator can configure the out-of-the-box `NumberOneToBooleanConverter` custom converter that would either map all `NUMBER(1)` columns to a `BOOLEAN` or if the `selector` parameter is set,
then a subset of columns could be enumerated using a comma-separated list of regular expressions.

Following is an example configuration:

[source]
----
converters=boolean
boolean.type=io.debezium.connector.oracle.converters.NumberOneToBooleanConverter
boolean.selector=.*MYTABLE.FLAG,.*.IS_ARCHIVED
----

[id="oracle-decimal-types"]
=== Decimal types
The setting of the Oracle connector configuration property, `decimal.handling.mode` determines how the connector maps decimal types.

When the `decimal.handling.mode` property is set to `precise`, the connector uses Kafka Connect `org.apache.kafka.connect.data.Decimal` logical type for all `DECIMAL` and `NUMERIC` columns.
This is the default mode.

However, when the `decimal.handling.mode` property is set to `double`, the connector will represent the values as Java double values with schema type `FLOAT64`.

The last possible setting for the `decimal.handling.mode` configuration property is `string`.
In this case, the connector reprsents `DECIMAL` and `NUMERIC` values as their formatted string representation with schema type `STRING`.

[id="oracle-temporal-types"]
=== Temporal types

Other than Oracle's `INTERVAL`, `TIMESTAMP WITH TIME ZONE` and `TIMESTAMP WITH LOCAL TIME ZONE` data types, the other temporal types depend on the value of the `time.precision.mode` configuration property.

When the `time.precision.mode` configuration property is set to `adaptive` (the default), then the connector will determine the literal and semantic type for the temporal types based on the column's data type definition so that events _exactly_ represent the values in the database:

[cols="25%a,20%a,55%a",options="header"]
|===
|Oracle data type |Literal type (schema type) |Semantic type (schema name) and Notes

|`DATE`
|`INT64`
|`io.debezium.time.Timestamp` +
 +
Represents the number of milliseconds past epoch, and does not include timezone information.

|`INTERVAL DAY[(M)] TO SECOND`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average.

|`INTERVAL YEAR[(M)] TO MONTH`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average.

|`TIMESTAMP(0 - 3)`
|`INT64`
|`io.debezium.time.Timestamp` +
 +
Represents the number of milliseconds past epoch, and does not include timezone information.

|`TIMESTAMP, TIMESTAMP(4 - 6)`
|`INT64`
|`io.debezium.time.MicroTimestamp` +
 +
Represents the number of microseconds past epoch, and does not include timezone information.

|`TIMESTAMP(7 - 9)`
|`INT64`
|`io.debezium.time.NanoTimestamp` +
 +
Represents the number of nanoseconds past epoch, and does not include timezone information.

|`TIMESTAMP WITH TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp with timezone information.

|`TIMESTAMP WITH LOCAL TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp in UTC.

|===

When the `time.precision.mode` configuration property is set to `connect`, then the connector will use the predefined Kafka Connect logical types.
This may be useful when consumers only know about the built-in Kafka Connect logical types and are unable to handle variable-precision time values.
Since Oracle supports precision that exceeds what Kafka Connect's logical types support, using `connect` time precision will *result in a loss of precision* when the database column has a _fractional second precision_ value that is greater than 3:

[cols="25%a,20%a,55%a",options="header"]
|===
|Oracle data type |Literal type (schema type) |Semantic type (schema name) and Notes

|`DATE`
|`INT32`
|`org.apache.kafka.connect.data.Date` +
 +
Represents the number of days since the epoch.

|`INTERVAL DAY[(M)] TO SECOND`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average.

|`INTERVAL YEAR[(M)] TO MONTH`
|`FLOAT64`
|`io.debezium.time.MicroDuration` +
 +
The number of micro seconds for a time interval using the `365.25 / 12.0` formula for days per month average.

|`TIMESTAMP(0 - 3)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since epoch, and does not include timezone information.

|`TIMESTAMP(4 - 6)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since epoch, and does not include timezone information.

|`TIMESTAMP(7 - 9)`
|`INT64`
|`org.apache.kafka.connect.data.Timestamp` +
 +
Represents the number of milliseconds since epoch, and does not include timezone information.

|`TIMESTAMP WITH TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp with timezone information.

|`TIMESTAMP WITH LOCAL TIME ZONE`
|`STRING`
|`io.debezium.time.ZonedTimestamp` +
 +
A string representation of a timestamp in UTC.

|===

[[setting-up-oracle]]
== Setting up Oracle

The following database set up steps are necessary to use the {prodname} Oracle connector.
These steps assume the use of the multi-tenancy configuration with a container database and at least one pluggable database.
If you intend to not use multi-tenancy configuration, the following steps may require adjustment.

You can find a template for setting up Oracle in a virtual machine (via Vagrant) in the https://github.com/debezium/oracle-vagrant-box/[oracle-vagrant-box/] repository.

=== Preparing the Database

.Configuration needed for Oracle LogMiner
[source,indent=0]
----
ORACLE_SID=ORACLCDB dbz_oracle sqlplus /nolog

CONNECT sys/top_secret AS SYSDBA
alter system set db_recovery_file_dest_size = 10G;
alter system set db_recovery_file_dest = '/opt/oracle/oradta/recovery_area' scope=spfile;
shutdown immediate
startup mount
alter database archivelog;
alter database open;
-- Should now "Database log mode: Archive Mode"
archive log list

exit;
----

In addition, supplemental logging must be enabled for captured tables or the database in order for data changes to capture the _before_ state of changed database rows.
The following illustrates how to configure this on a specific table, which is the ideal choice to minimize the amount of information captured in the Oracle redo logs.

[source,indent=0]
----
ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
----

Minimal supplemental logging must be enabled at the database level and can be configured as follows.

[source,indent=0]
----
ALTER DATABASE ADD SUPPLEMENTAL LOG DATA;
----

=== Creating users for the connector

The {prodname} Oracle connector requires that users accounts be set up with specific permissions so that the connector can capture change events.
The following briefly describes these user configurations using a multi-tenant database model.

[[oracle-create-users-logminer]]
.Creating the connector's LogMiner user
[source,indent=0]
----
sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
  CREATE TABLESPACE logminer_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLPDB1 as sysdba
  CREATE TABLESPACE logminer_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba

  CREATE USER c##dbzuser IDENTIFIED BY dbz
    DEFAULT TABLESPACE logminer_tbs
    QUOTA UNLIMITED ON logminer_tbs
    CONTAINER=ALL;

  GRANT CREATE SESSION TO c##dbzuser CONTAINER=ALL;
  GRANT SET CONTAINER TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$DATABASE to c##dbzuser CONTAINER=ALL;
  GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ANY TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;
  GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ANY TRANSACTION TO c##dbzuser CONTAINER=ALL;
  GRANT LOGMINING TO c##dbzuser CONTAINER=ALL;

  GRANT CREATE TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT LOCK ANY TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT ALTER ANY TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT CREATE SEQUENCE TO c##dbzuser CONTAINER=ALL;

  GRANT EXECUTE ON DBMS_LOGMNR TO c##dbzuser CONTAINER=ALL;
  GRANT EXECUTE ON DBMS_LOGMNR_D TO c##dbzuser CONTAINER=ALL;

  GRANT SELECT ON V_$LOG TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$LOG_HISTORY TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$LOGMNR_LOGS TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$LOGMNR_CONTENTS TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$LOGMNR_PARAMETERS TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$LOGFILE TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$ARCHIVED_LOG TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$ARCHIVE_DEST_STATUS TO c##dbzuser CONTAINER=ALL;

  exit;
----

[[oracle-deploying-a-connector]]
== Deploying a Connector

Due to licensing requirements, the {prodname} Oracle Connector does not ship with the Oracle JDBC driver and the XStream API JAR.
You can obtain them for free by downloading the https://www.oracle.com/database/technologies/instant-client/downloads.html[Oracle Instant Client].

Extract the archive and navigate to the Instant Client directory.
Copy `ojdbc<version>.jar` and `xstreams.jar` into Kafka's _libs_ directory.
Lastly, create an environment variable, `LD_LIBRARY_PATH`, that points to the instant client directory, as shown below:

[source,bash,indent=0]
----
LD_LIBRARY_PATH=/path/to/instant_client/
----

[[oracle-example-configuration]]
=== Example Configuration

The following shows an example JSON request for registering an instance of the {prodname} Oracle connector:

[source,json,indent=0]
----
{
    "name": "inventory-connector",
    "config": {
        "connector.class" : "io.debezium.connector.oracle.OracleConnector",
        "tasks.max" : "1",
        "database.server.name" : "server1",
        "database.hostname" : "<oracle ip>",
        "database.port" : "1521",
        "database.user" : "c##dbzuser",
        "database.password" : "dbz",
        "database.dbname" : "ORCLCDB",
        "database.pdb.name" : "ORCLPDB1",
        "database.history.kafka.bootstrap.servers" : "kafka:9092",
        "database.history.kafka.topic": "schema-changes.inventory"
    }
}
----

When using a more complex Oracle deployment or needing to use TNS names, then a raw JDBC url can be provided instead of a single hostname-port pair. Here is a similar example but that just passes the raw jdbc url:

[source,json,indent=0]
----
{
    "name": "inventory-connector",
    "config": {
        "connector.class" : "io.debezium.connector.oracle.OracleConnector",
        "tasks.max" : "1",
        "database.server.name" : "server1",
        "database.user" : "c##dbzuser",
        "database.password" : "dbz",
        "database.url": "jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(LOAD_BALANCE=OFF)(FAILOVER=ON)(ADDRESS=(PROTOCOL=TCP)(HOST=<oracle ip 1>)(PORT=1521))(ADDRESS=(PROTOCOL=TCP)(HOST=<oracle ip 2>)(PORT=1521)))(CONNECT_DATA=SERVICE_NAME=)(SERVER=DEDICATED)))",
        "database.dbname" : "ORCLCDB",
        "database.pdb.name" : "ORCLPDB1",
        "database.history.kafka.bootstrap.servers" : "kafka:9092",
        "database.history.kafka.topic": "schema-changes.inventory"
    }
}
----

[[oracle-database-mode]]
=== Pluggable vs Non-Pluggable databases

The {prodname} Oracle connector supports both deployment practices of pluggable databases (CDB mode) as well as non-pluggable databases (non-CDB mode).

.Example configuration using non-CDB (Non Pluggable Database) installations
[source,json,indent=0]
----
{
  "config": {
    "connector.class" : "io.debezium.connector.oracle.OracleConnector",
    "tasks.max" : "1",
    "database.server.name" : "server1",
    "database.hostname" : "<oracle ip>",
    "database.port" : "1521",
    "database.user" : "c##dbzuser",
    "database.password" : "dbz",
    "database.dbname" : "ORCLCDB",
    "database.history.kafka.bootstrap.servers" : "kafka:9092",
    "database.history.kafka.topic": "schema-changes.inventory"
  }
}
----

.Example configuration using CDB (Pluggable Database) installations
[source,json,indent=0]
----
{
  "config": {
    "connector.class" : "io.debezium.connector.oracle.OracleConnector",
    "tasks.max" : "1",
    "database.server.name" : "server1",
    "database.hostname" : "<oracle ip>",
    "database.port" : "1521",
    "database.user" : "c##dbzuser",
    "database.password" : "dbz",
    "database.dbname" : "ORCLCDB",
    "database.pdb.name" : "ORCLPDB1",
    "database.history.kafka.bootstrap.servers" : "kafka:9092",
    "database.history.kafka.topic": "schema-changes.inventory"
  }
}
----

[WARNING]
====
When using CDB installations, specify `database.pdb.name`. +
When using a non-CDB installation, do *not* specify the `database.pdb.name`.
====

[[oracle-connector-properties]]
=== Connector Properties

The following configuration properties are _required_ unless a default value is available.

[cols="30%a,25%a,45%a"]
|===
|Property
|Default
|Description

|[[oracle-property-name]]<<oracle-property-name, `+name+`>>
|
|Unique name for the connector. Attempting to register again with the same name will fail. (This property is required by all Kafka Connect connectors.)

|[[oracle-property-connector-class]]<<oracle-property-connector-class, `+connector.class+`>>
|
|The name of the Java class for the connector. Always use a value of `io.debezium.connector.oracle.OracleConnector` for the Oracle connector.

|[[oracle-property-tasks-max]]<<oracle-property-tasks-max, `+tasks.max+`>>
|`1`
|The maximum number of tasks that should be created for this connector. The Oracle connector always uses a single task and therefore does not use this value, so the default is always acceptable.

|[[oracle-property-database-hostname]]<<oracle-property-database-hostname, `+database.hostname+`>>
|
|IP address or hostname of the Oracle database server.

|[[oracle-property-database-port]]<<oracle-property-database-port, `+database.port+`>>
|
|Integer port number of the Oracle database server.

|[[oracle-property-database-user]]<<oracle-property-database-user, `+database.user+`>>
|
|Name of the user to use when connecting to the Oracle database server.

|[[oracle-property-database-password]]<<oracle-property-database-password, `+database.password+`>>
|
|Password to use when connecting to the Oracle database server.

|[[oracle-property-database-dbname]]<<oracle-property-database-dbname, `+database.dbname+`>>
|
|Name of the database to connect to. Must be the CDB name when working with the CDB + PDB model.

|[[oracle-property-database-url]]<<oracle-property-database-url, `+database.url+`>>
|
|Raw database jdbc url. This property can be used when more flexibility is needed and can support raw TNS names or RAC connection strings.

|[[oracle-property-database-pdb-name]]<<oracle-property-database-pdb-name, `+database.pdb.name+`>>
|
|Name of the PDB to connect to, when working with the CDB + PDB model.

|[[oracle-property-database-server-name]]<<oracle-property-database-server-name, `+database.server.name+`>>
|
|Logical name that identifies and provides a namespace for the particular Oracle database server being monitored. The logical name should be unique across all other connectors, since it is used as a prefix for all Kafka topic names emanating from this connector.
Only alphanumeric characters, hyphens and underscores must be used.

|[[oracle-property-database-connection-adapter]]<<oracle-property-database-connection-adapter, `+database.connection.adapter+`>>
|`logminer`
|The adapter implementation to be used to stream database changes.
`logminer` (the default) to use the native Oracle LogMiner API;
`xstream` to use the Oracle XStreams API.

|[[oracle-property-snapshot-mode]]<<oracle-property-snapshot-mode, `+snapshot.mode+`>>
|_initial_
|A mode for taking an initial snapshot of the structure and optionally data of captured tables. Supported values are _initial_ (will take a snapshot of structure and data of captured tables; useful if topics should be populated with a complete representation of the data from the captured tables) and _schema_only_ (will take a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be propagated to topics). Once the snapshot is complete, the connector will continue reading change events from the database's redo logs.

|[[oracle-property-snapshot-locking-mode]]<<oracle-property-snapshot-locking-mode, `+snapshot.locking.mode+`>>
|_shared_
a|Controls whether and how long the connector holds a table lock, which prevents certain types of changes table operations, while the connector is performing a snapshot. Possible settings are: +
 +
`shared` - allows concurrent access to the table but prevents any session from acquiring a table exclusive lock (specifically, the connector will acquire a `ROW SHARE`level lock while capturing the schemas of the tables). +
 +
`none` - prevents the connector from acquiring any table locks during the snapshot. This setting is only safe to use if and _only_ if no schema changes are happening while the snapshot is running.

|[[oracle-property-snapshot-include-collection-list]]<<oracle-property-snapshot-include-collection-list, `+snapshot.include.collection.list+`>>
| All tables specified in `table.include.list`
|An optional, comma-separated list of regular expressions that match names of *fully-qualified* table names (`<db-name>.<schema-name>.<name>`) included in `table.include.list` for which you *want* to take the snapshot.

|[[oracle-property-snapshot-select-statement-overrides]]<<oracle-property-snapshot-select-statement-overrides, `+snapshot.select.statement.overrides+`>>
|
|Controls which rows from tables are included in snapshot. +
This property contains a comma-separated list of fully-qualified tables _(SCHEMA_NAME.TABLE_NAME)_. Select statements for the individual tables are specified in further configuration properties, one for each table, identified by the id `snapshot.select.statement.overrides.[SCHEMA_NAME].[TABLE_NAME]`. The value of those properties is the SELECT statement to use when retrieving data from the specific table during snapshotting. _A possible use case for large append-only tables is setting a specific point where to start (resume) snapshotting, in case a previous snapshotting was interrupted._ +
*Note*: This setting has impact on snapshots only. Events captured during log reading are not affected by it.

|[[oracle-property-schema-include-list]]<<oracle-property-schema-include-list, `+schema.include.list+`>>
|
|An optional, comma-separated list of regular expressions that match names of schemas for which you *want* to capture changes. Any schema name not included in `schema.include.list` is excluded from having its changes captured. By default, all non-system schemas have their changes captured. Do not also set the `schema.exclude.list` property.  When using LogMiner, only POSIX regular expressions are supported.

|[[oracle-property-schema-exclude-list]]<<oracle-property-schema-exclude-list, `+schema.exclude.list+`>>
|
|An optional, comma-separated list of regular expressions that match names of schemas for which you *do not* want to capture changes. Any schema whose name is not included in `schema.exclude.list` has its changes captured, with the exception of system schemas. Do not also set the `schema.include.list` property.  When using LogMiner, only POSIX regular expressions are supported.

|[[oracle-property-table-include-list]]<<oracle-property-table-include-list, `+table.include.list+`>>
|
|An optional comma-separated list of regular expressions that match fully-qualified table identifiers for tables to be monitored; any table not included in the include list will be excluded from monitoring. Each identifier is of the form _schemaName_._tableName_. By default the connector will monitor every non-system table in each monitored database. May not be used with `table.exclude.list`.  When using LogMiner, only POSIX regular expressions are supported.

|[[oracle-property-table-exclude-list]]<<oracle-property-table-exclude-list, `+table.exclude.list+`>>
|
|An optional comma-separated list of regular expressions that match fully-qualified table identifiers for tables to be excluded from monitoring; any table not included in the exclude list will be monitored. Each identifier is of the form _schemaName_._tableName_. May not be used with `table.include.list`.  When using LogMiner, only POSIX regular expressions are supported.

|[[oracle-property-column-include-list]]<<oracle-property-column-include-list, `+column.include.list+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of columns that should be included in the change event message values.
Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
Note that primary key columns are always included in the event's key, even if not included in the value.
Do not also set the `column.exclude.list` property.

|[[oracle-property-column-exclude-list]]<<oracle-property-column-exclude-list, `+column.exclude.list+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of columns that should be excluded from change event message values.
Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
Note that primary key columns are always included in the event's key, also if excluded from the value.
Do not also set the `column.include.list` property.

|[[oracle-property-column-mask-hash]]<<oracle-property-column-mask-hash, `+column.mask.hash._hashAlgorithm_.with.salt._salt_+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be pseudonyms in the change event message values with a field value consisting of the hashed value using the algorithm `_hashAlgorithm_` and salt `_salt_`.
Based on the used hash function referential integrity is kept while data is pseudonymized. Supported hash functions are described in the {link-java7-standard-names}[MessageDigest section] of the Java Cryptography Architecture Standard Algorithm Name Documentation.
The hash is automatically shortened to the length of the column.

Multiple properties with different lengths can be used in a single configuration, although in each the length must be a positive integer or zero. Fully-qualified names for columns are of the form _pdbName_._schemaName_._tableName_._columnName_.

Example:

    column.mask.hash.SHA-256.with.salt.CzQMA0cB5K = inventory.orders.customerName, inventory.shipment.customerName

where `CzQMA0cB5K` is a randomly selected salt.

Note: Depending on the `_hashAlgorithm_` used, the `_salt_` selected and the actual data set, the resulting masked data set may not be completely anonymized.

|[[oracle-property-decimal-handling-mode]]<<oracle-property-decimal-handling-mode, `+decimal.handling.mode+`>>
|`precise`
| Specifies how the connector should handle floating point values for `NUMBER`, `DECIMAL` and `NUMERIC` columns: `precise` (the default) represents them precisely using `java.math.BigDecimal` values represented in change events in a binary form; or `double` represents them using `double` values, which may result in a loss of precision but will be far easier to use. `string` option encodes values as formatted string which is easy to consume but a semantic information about the real type is lost. See <<oracle-decimal-types>>.

|[[oracle-property-event-processing-failure-handling-mode]]<<oracle-property-event-processing-failure-handling-mode, `+event.processing.failure.handling.mode+`>>
|`fail`
| Specifies how the connector should react to exceptions during processing of events.
`fail` will propagate the exception (indicating the offset of the problematic event), causing the connector to stop. +
`warn` will cause the problematic event to be skipped and the offset of the problematic event to be logged. +
`skip` will cause the problematic event to be skipped.

|[[oracle-property-max-queue-size]]<<oracle-property-max-queue-size, `+max.queue.size+`>>
|`8192`
|Positive integer value that specifies the maximum size of the blocking queue into which change events read from the database log are placed before they are written to Kafka. This queue can provide backpressure to the binlog reader when, for example, writes to Kafka are slower or if Kafka is not available. Events that appear in the queue are not included in the offsets periodically recorded by this connector. Defaults to 8192, and should always be larger than the maximum batch size specified in the `max.batch.size` property.

|[[oracle-property-max-batch-size]]<<oracle-property-max-batch-size, `+max.batch.size+`>>
|`2048`
|Positive integer value that specifies the maximum size of each batch of events that should be processed during each iteration of this connector. Defaults to 2048.

|[[oracle-property-max-queue-size-in-bytes]]<<oracle-property-max-queue-size-in-bytes, `+max.queue.size.in.bytes+`>>
|`0`
|Long value for the maximum size in bytes of the blocking queue. The feature is disabled by default, it will be active if it's set with a positive long value.

|[[oracle-property-poll-interval-ms]]<<oracle-property-poll-interval-ms, `+poll.interval.ms+`>>
|`1000`
|Positive integer value that specifies the number of milliseconds the connector should wait during each iteration for new change events to appear. Defaults to 1000 milliseconds, or 1 second.

|[[oracle-property-tombstones-on-delete]]<<oracle-property-tombstones-on-delete, `+tombstones.on.delete+`>>
|`true`
|Controls whether a _delete_ event is followed by a tombstone event. +
 +
`true` - a delete operation is represented by a _delete_ event and a subsequent tombstone event.  +
 +
`false` - only a _delete_ event is emitted. +
 +
After a source record is deleted, emitting a tombstone event (the default behavior) allows Kafka to completely delete all events that pertain to the key of the deleted row in case {link-kafka-docs}/#compaction[log compaction] is enabled for the topic.

|[[oracle-property-message-key-columns]]<<oracle-property-message-key-columns, `+message.key.columns+`>>
|
| A semi-colon list of regular expressions that match fully-qualified tables and columns to map a primary key. +
Each item (regular expression) must match the `<fully-qualified table>:<a comma-separated list of columns>` representing the custom key. +
Fully-qualified tables could be defined as _pdbName_._schemaName_._tableName_.

|[[oracle-property-column-truncate-to-length-chars]]<<oracle-property-column-truncate-to-length-chars, `+column.truncate.to._length_.chars+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be truncated in the change event message values if the field values are longer than the specified number of characters. Multiple properties with different lengths can be used in a single configuration, although in each the length must be a positive integer. Fully-qualified names for columns are of the form _pdbName_._schemaName_._tableName_._columnName_.

|[[oracle-property-column-mask-with-length-chars]]<<oracle-property-column-mask-with-length-chars, `+column.mask.with._length_.chars+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be replaced in the change event message values with a field value consisting of the specified number of asterisk (`*`) characters. Multiple properties with different lengths can be used in a single configuration, although in each the length must be a positive integer or zero. Fully-qualified names for columns are of the form _pdbName_._schemaName_._tableName_._columnName_.

|[[oracle-property-column-propagate-source-type]]<<oracle-property-column-propagate-source-type, `+column.propagate.source.type+`>>
|
|An optional comma-separated list of regular expressions that match the fully-qualified names of columns whose original type and length should be added as a parameter to the corresponding field schemas in the emitted change messages.
The schema parameters `pass:[_]pass:[_]debezium.source.column.type`, `pass:[_]pass:[_]debezium.source.column.length` and `pass:[_]pass:[_]debezium.source.column.scale` will be used to propagate the original type name and length (for variable-width types), respectively.
Useful to properly size corresponding columns in sink databases.
Fully-qualified names for columns are of the form _tableName_._columnName_, or _schemaName_._tableName_._columnName_.

|[[oracle-property-datatype-propagate-source-type]]<<oracle-property-datatype-propagate-source-type, `+datatype.propagate.source.type+`>>
|
|An optional comma-separated list of regular expressions that match the database-specific data type name of columns whose original type and length should be added as a parameter to the corresponding field schemas in the emitted change messages.
The schema parameters `pass:[_]pass:[_]debezium.source.column.type`, `pass:[_]pass:[_]debezium.source.column.length` and `pass:[_]pass:[_]debezium.source.column.scale` will be used to propagate the original type name and length (for variable-width types), respectively.
Useful to properly size corresponding columns in sink databases.
Fully-qualified data type names are of the form _tableName_._typeName_, or _schemaName_._tableName_._typeName_.
See the {link-prefix}:{link-oracle-connector}#oracle-data-type-mappings[list of Oracle-specific data type names].

|[[oracle-property-heartbeat-interval-ms]]<<oracle-property-heartbeat-interval-ms, `+heartbeat.interval.ms+`>>
|`0`
|Controls how frequently heartbeat messages are sent. +
This property contains an interval in milli-seconds that defines how frequently the connector sends messages into a heartbeat topic.
This can be used to monitor whether the connector is still receiving change events from the database.
You also should leverage heartbeat messages in cases where only records in non-captured tables are changed for a longer period of time.
In such situation the connector would proceed to read the log from the database but never emit any change messages into Kafka,
which in turn means that no offset updates will be committed to Kafka.
This will cause the redo log files to be retained by the database longer than needed
(as the connector actually has processed them already but never got a chance to flush the latest retrieved SCN to the database)
and also may result in more change events to be re-sent after a connector restart.
Set this parameter to `0` to not send heartbeat messages at all. +
Disabled by default.

|[[oracle-property-heartbeat-topics-prefix]]<<oracle-property-heartbeat-topics-prefix, `+heartbeat.topics.prefix+`>>
|`__debezium-heartbeat`
|Controls the naming of the topic to which heartbeat messages are sent. +
The topic is named according to the pattern `<heartbeat.topics.prefix>.<server.name>`.

|[[oracle-property-snapshot-delay-ms]]<<oracle-property-snapshot-delay-ms, `+snapshot.delay.ms+`>>
|
|An interval in milli-seconds that the connector should wait before taking a snapshot after starting up; +
Can be used to avoid snapshot interruptions when starting multiple connectors in a cluster, which may cause re-balancing of connectors.

|[[oracle-property-snapshot-fetch-size]]<<oracle-property-snapshot-fetch-size, `+snapshot.fetch.size+`>>
|`2000`
|Specifies the maximum number of rows that should be read in one go from each table while taking a snapshot.
The connector will read the table contents in multiple batches of this size. Defaults to 2000.

|[[oracle-property-sanitize-field-names]]<<oracle-property-sanitize-field-names, `+sanitize.field.names+`>>
|`true` when connector configuration explicitly specifies the `key.converter` or `value.converter` parameters to use Avro, otherwise defaults to `false`.
|Whether field names will be sanitized to adhere to Avro naming requirements.
See {link-prefix}:{link-avro-serialization}#avro-naming[Avro naming] for more details.

|[[oracle-property-provide-transaction-metadata]]<<oracle-property-provide-transaction-metadata, `+provide.transaction.metadata+`>>
|`false`
|When set to `true` {prodname} generates events with transaction boundaries and enriches data events envelope with transaction metadata.

See {link-prefix}:{link-oracle-connector}#oracle-transaction-metadata[Transaction Metadata] for additional details.

|[[oracle-property-log-mining-strategy]]<<oracle-property-log-mining-strategy, `+log.mining.strategy+`>>
|`redo_log_catalog`
|The mining strategy controls how Oracle LogMiner builds and uses a given data dictionary for resolving table and column ids to names. +
 +
`redo_log_catalog` - Writes the data dictionary to the online redo logs causing more archive logs to be generated over time.
This also enables tracking DDL changes against captured tables, so if the schema changes frequently this is the ideal choice. +
 +
`online_catalog` - Uses the database's current data dictionary to resolve object ids and does not write any extra information to the online redo logs.
This allows LogMiner to mine substantially faster but at the expense that DDL changes cannot be tracked.
If the captured table(s) schema changes infrequently or never, this is the ideal choice.

|[[oracle-property-log-mining-batch-size-min]]<<oracle-property-log-mining-batch-size-min, `+log.mining.batch.size.min+`>>
|`1000`
|The minimum SCN interval size that this connector will try to read from redo/archive logs. Active batch size will be also increased/decreased by this amount for tuning connector throughput when needed.

|[[oracle-property-log-mining-batch-size-max]]<<oracle-property-log-mining-batch-size-max, `+log.mining.batch.size.max+`>>
|`100000`
|The maximum SCN interval size that this connector will use when reading from redo/archive logs.

|[[oracle-property-log-mining-batch-size-default]]<<oracle-property-log-mining-batch-size-default, `+log.mining.batch.size.default+`>>
|`20000`
|The starting SCN interval size that the connector will use for reading data from redo/archive logs.

|[[oracle-property-log-mining-sleep-time-min-ms]]<<oracle-property-log-mining-sleep-time-min-ms, `+log.mining.sleep.time.min.ms+`>>
|`0`
|The minimum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.

|[[oracle-property-log-mining-sleep-time-max-ms]]<<oracle-property-log-mining-sleep-time-max-ms, `+log.mining.sleep.time.max.ms+`>>
|`3000`
|The maximum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.

|[[oracle-property-log-mining-sleep-time-default-ms]]<<oracle-property-log-mining-sleep-time-default-ms, `+log.mining.sleep.time.default.ms+`>>
|`1000`
|The starting amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.

|[[oracle-property-log-mining-sleep-time-increment-ms]]<<oracle-property-log-mining-sleep-time-increment-ms, `+log.mining.sleep.time.increment.ms+`>>
|`200`
|The maximum amount of time up or down that the connector will use to tune the optimal sleep time when reading data from logminer. Value is in milliseconds.

|[[oracle-property-log-mining-view-fetch-size]]<<oracle-property-log-mining-view-fetch-size, `+log.mining.view.fetch.size+`>>
|`10000`
|The number of content records that will be fetched from the log miner content view.

|[[oracle-property-log-mining-archive-log-hours]]<<oracle-property-log-mining-archive-log-hours, `+log.mining.archive.log.hours+`>>
|`0`
|The number of hours in the past from SYSDATE to mine archive logs.
Using the default `0` will mine all archive logs.

|[[oracle-property-log-mining-transaction-retention-hours]]<<oracle-property-log-mining-transaction-retention-hours, `log.mining.transaction.retention.hours`>>
|`0`
|Positive integer value that specifies the number of hours to retain long running transactions between redo log switches.
When set to `0`, transactions are retained until a commit or rollback is detected.

The LogMiner adapter maintains an in-memory buffer of all running transactions.
As all DML operations that are part of a transaction will be buffered until a commit or rollback is detected,
long-running transactions should be avoided in order to not overflow that buffer.
Any transaction that exceeds this configured value will be discarded entirely and no messages emitted for the operations that were part of the transaction.

While this option allows the behavior to be configured on a case-by-case basis,
we have plans to enhance this behavior in a future release by means of adding a scalable transaction buffer, (see {link-prefix}:{jira-url}/browse/DBZ-3123[DBZ-3123]).

|[[oracle-property-rac-nodes]]<<oracle-property-rac-nodes, `+rac.nodes+`>>
|
|A comma-separated list of RAC node host names or addresses.
This field is required to enable Oracle RAC support.

|[[oracle-property-skipped-operations]]<<oracle-property-skipped-operations, `+skipped.operations+`>>
|
| comma-separated list of operation types that will be skipped during streaming.
The operations include: `c` for inserts/create, `u` for updates, and `d` for deletes.
By default, no operations are skipped.

|===

[id="{context}-pass-through-configuration-properties-for-database-drivers"]
==== Pass-through configuration properties for database drivers

The {prodname} connector provides for pass-through configuration of the database driver.
Pass-through database properties begin with the prefix `database.*`.
For example, the property `database.tinyInt1isBit=false` is passed to the JDBC URL.
Debezium strips the prefix before it passes the property to the database driver.

[id="{context}-passthrough-database-history-properties-for-configuring-producer-and-consumer-clients"]
==== Pass-through database history properties for configuring producer and consumer clients

{prodname} relies on a Kafka producer to write schema changes to database history topics, and a Kafka consumer to read from database history topics when a connector starts.
You define the configuration for the Kafka producer and consumer clients by assigning values to a set of pass-through configuration properties that begin with the `database.history.producer.\*` and `database.history.consumer.*` prefixes.
The pass-through producer and consumer database history properties control a range of behaviors, such as how these clients secure connections with the Kafka broker, as shown in the following example:

[source,indent=0]
----
database.history.producer.security.protocol=SSL
database.history.producer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
database.history.producer.ssl.keystore.password=test1234
database.history.producer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
database.history.producer.ssl.truststore.password=test1234
database.history.producer.ssl.key.password=test1234
database.history.consumer.security.protocol=SSL
database.history.consumer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
database.history.consumer.ssl.keystore.password=test1234
database.history.consumer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
database.history.consumer.ssl.truststore.password=test1234
database.history.consumer.ssl.key.password=test1234
----

As is the case with the xref:{context}-pass-through-configuration-properties-for-database-drivers[pass-through properties for database drivers], {prodname} strips the prefixes from the properties before {prodname} it passes them to configure the database history producer or consumer client.

See the Kafka documentation for more details about link:https://kafka.apache.org/documentation.html#producerconfigs[Kafka producer configuration properties] and link:https://kafka.apache.org/documentation.html#consumerconfigs[Kafka consumer configuration properties].

[id="{context}-database-history-properties-for-configuring-connector-behavior"]
==== Database history properties for configuring connector behavior

In addition to the pass-through database history properties that you use to configure producers or consumers, there is a set of `database.history.*` properties whose names do not include `producer` or `consumer`.
You use these `database.history` properties to define how the {context} connector processes event data that it reads from the schema history.

The following table describes the `database.history` properties for configuring the {prodname} {context} connector.

include::{partialsdir}/modules/all-connectors/ref-connector-configuration-database-history-properties.adoc[leveloffset=+1]


[[oracle-monitoring]]
== Monitoring

The {prodname} Oracle connector has three metric types in addition to the built-in support for JMX metrics that Zookeeper, Kafka, and Kafka Connect have.

* <<oracle-snapshot-metrics, snapshot metrics>>; for monitoring the connector when performing snapshots
* <<oracle-streaming-metrics, streaming metrics>>; for monitoring the connector when processing change events
* <<oracle-schema-history-metrics, schema history metrics>>; for monitoring the status of the connector's schema history

Please refer to the {link-prefix}:{link-debezium-monitoring}#monitoring-debezium[monitoring documentation] for details of how to expose these metrics via JMX.

[[oracle-monitoring-snapshots]]
[[oracle-snapshot-metrics]]
=== Snapshot Metrics

The *MBean* is `debezium.oracle:type=connector-metrics,context=snapshot,server=_<database.server.name>_`.

include::{partialsdir}/modules/all-connectors/ref-connector-monitoring-snapshot-metrics.adoc[leveloffset=+1]

[[oracle-monitoring-streaming]]
[[oracle-streaming-metrics]]
=== Streaming Metrics

The *MBean* is `debezium.oracle:type=connector-metrics,context=streaming,server=_<database.server.name>_`.

include::{partialsdir}/modules/all-connectors/ref-connector-monitoring-streaming-metrics.adoc[leveloffset=+1]

The {prodname} Oracle connector also provides the following additional streaming metrics:

.Descriptions of additional streaming metrics
[cols="45%a,25%a,30%a"]
|===
|Attributes |Type |Description

|[[oracle-streaming-metrics-currentscn]]<<oracle-streaming-metrics-currentscn, `+CurrentScn+`>>
|`string`
|The most recent system change number that has been processed.

|[[oracle-streaming-metrics-oldest-scn]]<<oracle-streaming-metrics-oldest-scn, `+OldestScn+`>>
|`string`
|The oldest system change number in the transaction buffer.

|[[oracle-streaming-metrics-committed-scn]]<<oracle-streaming-metrics-committed-scn, `+ComittedScn+`>>
|`string`
|The last committed system change number from the transaction buffer.

|[[oracle-streaming-metrics-offset-scn]]<<oracle-streaming-metrics-offset-scn, `+OffsetScn+`>>
|`string`
|The system change number currently written to the connector's offsets.

|[[oracle-streaming-metrics-currentredologfilename]]<<oracle-streaming-metrics-currentredologfilename, `+CurrentRedoLogFileName+`>>
|`string[]`
|Array of the log files that are currently mined.

|[[oracle-streaming-metrics-minimumminedlogcount]]<<oracle-streaming-metrics-minimumminedlogcount, `+MinimumMinedLogCount+`>>
|`long`
|The minimum number of logs specified for any LogMiner session.

|[[oracle-streaming-metrics-maximumminedlogcount]]<<oracle-streaming-metrics-maximumminedlogcount, `+MaximumMinedLogCount+`>>
|`long`
|The maximum number of logs specified for any LogMiner session.

|[[oracle-streaming-metrics-redologstatus]]<<oracle-streaming-metrics-redologstatus, `+RedoLogStatus+`>>
|`string[]`
|Array of the current state for each mined logfile with the format `_filename_\|_status_`.

|[[oracle-streaming-metrics-switchcounter]]<<oracle-streaming-metrics-switchcounter, `+SwitchCounter+`>>
|`int`
|The number of times the database has performed a log switch for the last day.

|[[oracle-streaming-metrics-lastcaptureddmlcount]]<<oracle-streaming-metrics-lastcaptureddmlcount, `+LastCapturedDmlCount+`>>
|`long`
|The number of DML operations observed in the last LogMiner session query.

|[[oracle-streaming-metrics-maxcaptureddmlinbatch]]<<oracle-streaming-metrics-maxcaptureddmlinbatch, `+MaxCapturedDmlInBatch+`>>
|`long`
|The maximum number of DML operations observed while processing a single LogMiner session query.

|[[oracle-streaming-metrics-totalcaptureddmlcount]]<<oracle-streaming-metrics-totalcaptureddmlcount, `+TotalCapturedDmlCount+`>>
|`long`
|The total number of DML operations observed.

|[[oracle-streaming-metrics-fetchingquerycount]]<<oracle-streaming-metrics-fetchingquerycount, `+FetchingQueryCount+`>>
|`long`
|The total number of LogMiner session query (aka batches) performed.

|[[oracle-streaming-metrics-lastdurationoffetchqueryinmilliseconds]]<<oracle-streaming-metrics-lastdurationoffetchqueryinmilliseconds, `+LastDurationOfFetchQueryInMilliseconds+`>>
|`long`
|The duration of the last LogMiner session query's fetch in milliseconds.

|[[oracle-streaming-metrics-maxdurationoffetchqueryinmilliseconds]]<<oracle-streaming-metrics-maxdurationoffetchqueryinmilliseconds, `+MaxDurationOfFetchQueryInMilliseconds+`>>
|`long`
|The maximum duration of any LogMiner session query's fetch in milliseconds.

|[[oracle-streaming-metrics-lastbatchprocessingtimeinmilliseconds]]<<oracle-streaming-metrics-lastbatchprocessingtimeinmilliseconds, `+LastBatchProcessingTimeInMilliseconds+`>>
|`long`
|The duration for processing the last LogMiner query batch results in milliseconds.

|[[oracle-streaming-metrics-totalparsetimeinmilliseconds]]<<oracle-streaming-metrics-totalparsetimeinmilliseconds, `+TotalParseTimeInMilliseconds+`>>
|`long`
|The time in milliseconds spent parsing DML event SQL sattements.

|[[oracle-streaming-metrics-lastminingsessionstarttimeinmilliseconds]]<<oracle-streaming-metrics-lastminingsessionstarttimeinmilliseconds, `+LastMiningSessionStartTimeInMilliseconds+`>>
|`long`
|The duration in milliseconds to start the last LogMiner session.

|[[oracle-streaming-metrics-maxminingsessionstarttimeinmilliseconds]]<<oracle-streaming-metrics-maxminingsessionstarttimeinmilliseconds, `+MaxMiningSessionStartTimeInMilliseconds+`>>
|`long`
|The longest duration in milliseconds to start a LogMiner session.

|[[oracle-streaming-metrics-totalminingsessionstarttimeinmilliseconds]]<<oracle-streaming-metrics-totalminingsessionstarttimeinmilliseconds, `+TotalMiningSessionStartTimeInMilliseconds+`>>
|`long`
|The total duration in milliseconds spent by the connector starting LogMiner sessions.

|[[oracle-streaming-metrics-minbatchprocessingtimeinmilliseconds]]<<oracle-streaming-metrics-minbatchprocessingtimeinmilliseconds, `+MinBatchProcessingTimeInMilliseconds+`>>
|`long`
|The minimum duration in milliseconds spent processing results from a single LogMiner session.

|[[oracle-streaming-metrics-maxbatchprocessingtimeinmilliseconds]]<<oracle-streaming-metrics-maxbatchprocessingtimeinmilliseconds, `+MaxBatchProcessingTimeInMilliseconds+`>>
|`long`
|The maximum duration in milliseconds spent processing results from a single LogMiner session.

|[[oracle-streaming-metrics-totalprocessingtimeinmilliseconds]]<<oracle-streaming-metrics-totalprocessingtimeinmilliseconds, `+TotalProcessingTimeInMilliseconds+`>>
|`long`
|The total duration in milliseconds spent processing results from LogMiner sessions.

|[[oracle-streaming-metrics-totalresultsetnexttimeinmilliseconds]]<<oracle-streaming-metrics-totalresultsetnexttimeinmilliseconds, `+TotalResultSetNextTimeInMilliseconds+`>>
|`long`
|The total duration in milliseconds spent by the JDBC driver fetching the next row to be processed from the log mining view.

|[[oracle-streaming-metrics-totalprocessedrows]]<<oracle-streaming-metrics-totalprocessedrows, `+TotalProcessedRows+`>>
|`long`
|The total number of rows processed from the log mining view across all sessions.

|[[oracle-streaming-metrics-batchsize]]<<oracle-streaming-metrics-batchsize, `+BatchSize+`>>
|`int`
|The number of entries fetched by the log mining query per database round-trip.

|[[oracle-streaming-metrics-millisecondtosleepbetweenminingquery]]<<oracle-streaming-metrics-millisecondtosleepbetweenminingquery, `+MillisecondToSleepBetweenMiningQuery+`>>
|`long`
|The number of milliseconds the connector sleeps before fetching another batch of results from the log mining view.

|[[oracle-streaming-metrics-maxbatchprocessingthroughput]]<<oracle-streaming-metrics-maxbatchprocessingthroughput, `+MaxBatchProcessingThroughput+`>>
|`long`
|The maximum number of rows/second processed from the log mining view.

|[[oracle-streaming-metrics-averagebatchprocessingthroughput]]<<oracle-streaming-metrics-averagebatchprocessingthroughput, `+AverageBatchProcessingThroughput+`>>
|`long`
|The average number of rows/second processed from the log mining.

|[[oracle-streaming-metrics-lastbatchprocessingthroughput]]<<oracle-streaming-metrics-lastbatchprocessingthroughput, `+LastBatchProcessingThroughput+`>>
|`long`
|The average number of rows/second processed from the log mining view for the last batch.

|[[oracle-streaming-metrics-networkconnectionproblemscounter]]<<oracle-streaming-metrics-networkconnectionproblemscounter, `+NetworkConnectionProblemsCounter+`>>
|`long`
|The number of connection problems detected.

|[[oracle-streaming-metrics-hourstokeeptransactioninbuffer]]<<oracle-streaming-metrics-hourstokeeptransactioninbuffer, `+HoursToKeepTransactionInBuffer+`>>
|`int`
|The number of hours that transactions will be retained by the connector's in-memory buffer without being committed or rolled back before being discarded.
See <<oracle-property-log-mining-transaction-retention-hours, `log.mining.transaction.retention`>> for more details.

|[[oracle-streaming-metrics-recordmininghistory]]<<oracle-streaming-metrics-recordmininghistory, `+RecordMiningHistory+`>>
|`boolean`
|Flag that indicates if the mining results are being recorded.
See <<oracle-property-log-mining-history-recorder-class, `log.mining.history.recorder.class`>> for more details.

|[[oracle-streaming-metrics-number-of-active-transactions]]<<oracle-streaming-metrics-number-of-active-transactions, `+NumberOfActiveTransactions+`>>
|`long`
|The number of current active transactions in the transaction buffer.

|[[oracle-streaming-metrics-number-of-committed-transactions]]<<oracle-streaming-metrics-number-of-committed-transactions, `+NumberOfCommittedTransactions+`>>
|`long`
|The number of committed transactions in the transaction buffer.

|[[oracle-streaming-metrics-number-of-rolledback-transactions]]<<oracle-streaming-metrics-number-of-rolledback-transactions, `+NumberOfRolledBackTransactions+`>>
|`long`
|The number of rolled back transactions in the transaction buffer.

|[[oracle-streaming-metrics-commit-throughput]]<<oracle-streaming-metrics-commit-throughput, `+CommitThroughput+`>>
|`long`
|The average number of committed transactions per second in the transaction buffer.

|[[oracle-streaming-metrics-registered-dml-count]]<<oracle-streaming-metrics-registered-dml-count, `+RegisteredDmlCount+`>>
|`long`
|The number of registered DML operations in the transaction buffer.

|[[oracle-streaming-metrics-lag-from-source-in-milliseconds]]<<oracle-streaming-metrics-lag-from-source-in-milliseconds, `+LagFromSourceInMilliseconds+`>>
|`long`
|The time difference in milliseconds between when a change occurred in the transaction logs and when its added to the transaction buffer.

|[[oracle-streaming-metrics-max-lag-from-source-in-milliseconds]]<<oracle-streaming-metrics-max-lag-from-source-in-milliseconds, `+MaxLagFromSourceInMilliseconds+`>>
|`long`
|The maximum time difference in milliseconds between when a change occurred in the transaction logs and when its added to the transaction buffer.

|[[oracle-streaming-metrics-min-lag-from-source-in-milliseconds]]<<oracle-streaming-metrics-min-lag-from-source-in-milliseconds, `+MinLagFromSourceInMilliseconds+`>>
|`long`
|The minimum time difference in milliseconds between when a change occurred in the transaction logs and when its added to the transaction buffer.

|[[oracle-streaming-metrics-abandoned-transaction-ids]]<<oracle-streaming-metrics-abandoned-transaction-ids, `+AbandonedTransactionIds+`>>
|`string[]`
|An array of abandoned transaction identifiers removed from the transaction buffer due to their age.
See <<oracle-property-log-mining-transaction-retention-hours, `log.mining.transaction.retention.hours`>> for details.

|[[oracle-streaming-metrics-rolled-back-transaction-ids]]<<oracle-streaming-metrics-rolled-back-transaction-ids, `+RolledBackTransactionIds+`>>
|`string[]`
|An array of transaction identifiers that have been mined and rolled back in the transaction buffer.

|[[oracle-streaming-metrics-last-commit-duration-in-milliseconds]]<<oracle-streaming-metrics-last-commit-duration-in-milliseconds, `+LastCommitDurationInMilliseconds+`>>
|`long`
|The duration of the last transaction buffer commit operation in milliseconds.

|[[oracle-streaming-metrics-max-commit-duration-in-milliseconds]]<<oracle-streaming-metrics-max-commit-duration-in-milliseconds, `+MaxCommitDurationInMilliseconds+`>>
|`long`
|The duration of the longest transaction buffer commit operation in milliseconds.

|[[oracle-streaming-metrics-error-count]]<<oracle-streaming-metrics-error-count, `+ErrorCount+`>>
|`int`
|The number of errors detected.

|[[oracle-streaming-metrics-warning-count]]<<oracle-streaming-metrics-warning-count, `+WarningCount+`>>
|`int`
|The number of warnings detected.

|[[oracle-streaming-metrics-scn-freeze-count]]<<oracle-streaming-metrics-scn-freeze-count, `+ScnFreezeCount+`>>
|`int`
|The number of times the system change number has been checked for advancement and remains unchanged.
This is an indicator that long-running transaction(s) are ongoing and preventing the connector from flushing the latest processed system change number to the connector's offsets.
Under optimal operations, this should always be or remain close to `0`.

|[[oracle-streaming-metrics-unparsable-ddl-count]]<<oracle-streaming-metrics-unparsable-ddl-count, `+UnparsableDdlCount+`>>
|`int`
|The number of DDL records that have been detected but could not be parsed by the DDL parser.
This should always be `0`; however when allowing unparsable DDL to be skipped, this metric can be used to determine if any warnings have been written to the connector logs.

|===

[[oracle-monitoring-schema-history]]
[[oracle-schema-history-metrics]]
=== Schema History Metrics

The *MBean* is `debezium.mysql:type=connector-metrics,context=schema-history,server=_<database.server.name>_`.

include::{partialsdir}/modules/all-connectors/ref-connector-monitoring-schema-history-metrics.adoc[leveloffset=+1]

[[surrogate-schema-evolution]]
== Surrogate schema evolution

The Oracle connector will automatically track and apply table schema changes by parsing DDL from the redo logs.
In the event that the DDL parser encounters an unsupported statement, the connector offers an alternative way to handle applying the schema change should the need arise.

By default, the connector will stop when an unparseable DDL statement is encountered.
This DDL can be manually performed by using {prodname} link:/documentation/reference/configuration/signalling[signalling] to trigger the update of the database schema.

The type of the schema update action is `schema-changes`.
It will update the schema of all tables enumerated in the signal parameters.
The message does not contain the update to the schema, but the complete new schema structure.

.Action parameters
[cols="3,9",options="header"]
|===
|Name | Description

|`database`
|The name of the Oracle database.

|`schema`
|The name of the schema where changes are applied.

|`changes`
|An array containing the requested schema updates.

|`changes.type`
|Type of the schema change, usually `ALTER`

|`changes.id`
|The fully-qualified name of the table

|`changes.table`
|The fully-qualified name of the table

|`changes.table.defaultCharsetName`
|The character set name used for the table if different from database default

|`changes.table.primaryKeyColumnNames`
|Array with the name of columns composing the primary key

|`changes.table.columns`
|Array with the column metadata

|`...columns.name`
|The name of the column

|`...columns.jdbcType`
|The JDBC type of the column as defined at link:https://docs.oracle.com/javase/8/docs/api/java/sql/Types.html[JDBC API]

|`...columns.typeName`
|The name of the column type

|`...columns.typeExpression`
|The full column type definition

|`...columns.charsetName`
|The column character set if different from the default

|`...columns.length`
|The length/size constraint of the column

|`...columns.scale`
|The scale of numeric column

|`...columns.position`
|The position of the column in the table starting with `1`

|`...columns.optional`
|Boolean `true` if column value is not mandatory

|`...columns.autoIncremented`
|Boolean `true` if column value is automatically calculated from a sequence

|`...columns.generated`
|Boolean `true` if column value is automatically calculated

|===

Once the `schema-changes` signal has been inserted, the connector will need to be restarted with an altered configuration that includes specifying the  <<oracle-property-database-history-skip-unparseable-ddl, `+database.history.skip.unparseable.ddl+`>> option as `true`.
Once the connector's commit SCN has advanced beyond the DDL change, it's recommended that the connector's configuration be returned to its previous state so that unparseable DDL statements aren't skipped unexpectedly.

.Example of a logging record
[cols="1,9a",options="header"]
|===
|Column | Value

|id
|`924e3ff8-2245-43ca-ba77-2af9af02fa07`

|type
|`schema-changes`

|data
|[source,json,indent=0,subs="attributes"]
----
{
   "database":"ORCLPDB1",
   "schema":"DEBEZIUM",
   "changes":[
      {
         "type":"ALTER",
         "id":"\"ORCLPDB1\".\"DEBEZIUM\".\"CUSTOMER\"",
         "table":{
            "defaultCharsetName":null,
            "primaryKeyColumnNames":[
               "ID",
               "NAME"
            ],
            "columns":[
               {
                  "name":"ID",
                  "jdbcType":2,
                  "typeName":"NUMBER",
                  "typeExpression":"NUMBER",
                  "charsetName":null,
                  "length":9,
                  "scale":0,
                  "position":1,
                  "optional":false,
                  "autoIncremented":false,
                  "generated":false
               },
               {
                  "name":"NAME",
                  "jdbcType":12,
                  "typeName":"VARCHAR2",
                  "typeExpression":"VARCHAR2",
                  "charsetName":null,
                  "length":1000,
                  "position":2,
                  "optional":true,
                  "autoIncremented":false,
                  "generated":false
               },
               {
                  "name":"SCORE",
                  "jdbcType":2,
                  "typeName":"NUMBER",
                  "typeExpression":"NUMBER",
                  "charsetName":null,
                  "length":6,
                  "scale":2,
                  "position":3,
                  "optional":true,
                  "autoIncremented":false,
                  "generated":false
               },
               {
                  "name":"REGISTERED",
                  "jdbcType":93,
                  "typeName":"TIMESTAMP(6)",
                  "typeExpression":"TIMESTAMP(6)",
                  "charsetName":null,
                  "length":6,
                  "position":4,
                  "optional":true,
                  "autoIncremented":false,
                  "generated":false
               }
            ]
         }
      }
   ]
}
----

|===

[[oracle-xstreams-support]]
== XStreams support

The {prodname} Oracle connector by default ingests changes using native Oracle LogMiner.
The connector can be toggled to use Oracle XStream instead and to do so specific database and connector configurations must be used that differ from that of LogMiner.
In order to use the XStream API, you need to have a license for the GoldenGate product
(though it is not required that GoldenGate itself is installed).

=== Preparing the Database

.Configuration needed for Oracle XStream
[source,indent=0]
----
ORACLE_SID=ORCLCDB dbz_oracle sqlplus /nolog

CONNECT sys/top_secret AS SYSDBA
alter system set db_recovery_file_dest_size = 5G;
alter system set db_recovery_file_dest = '/opt/oracle/oradata/recovery_area' scope=spfile;
alter system set enable_goldengate_replication=true;
shutdown immediate
startup mount
alter database archivelog;
alter database open;
-- Should show "Database log mode: Archive Mode"
archive log list

exit;
----

In addition, supplemental logging must be enabled for captured tables or the database in order for data changes to capture the _before_ state of changed database rows.
The following illustrates how to configure this on a specific table, which is the ideal choice to minimize the amount of information captured in the Oracle redo logs.

[source,indent=0]
----
ALTER TABLE inventory.customers ADD SUPPLEMENTAL LOG DATA (ALL) COLUMNS;
----

=== Creating users for the connector

The {prodname} Oracle connector requires that users accounts be set up with specific permissions so that the connector can capture change events.
The following briefly describes these user configurations using a multi-tenant database model.


[[oracle-create-users-xstream]]
.Creating an XStream Administrator user
[source,indent=0]
----
sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
  CREATE TABLESPACE xstream_adm_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/xstream_adm_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLPDB1 as sysdba
  CREATE TABLESPACE xstream_adm_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/ORCLPDB1/xstream_adm_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
  CREATE USER c##dbzadmin IDENTIFIED BY dbz
    DEFAULT TABLESPACE xstream_adm_tbs
    QUOTA UNLIMITED ON xstream_adm_tbs
    CONTAINER=ALL;

  GRANT CREATE SESSION, SET CONTAINER TO c##dbzadmin CONTAINER=ALL;

  BEGIN
     DBMS_XSTREAM_AUTH.GRANT_ADMIN_PRIVILEGE(
        grantee                 => 'c##dbzadmin',
        privilege_type          => 'CAPTURE',
        grant_select_privileges => TRUE,
        container               => 'ALL'
     );
  END;
  /

  exit;
----

.Creating the connector's XStream user
[source,indent=0]
----
sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
  CREATE TABLESPACE xstream_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/xstream_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLPDB1 as sysdba
  CREATE TABLESPACE xstream_tbs DATAFILE '/opt/oracle/oradata/ORCLCDB/ORCLPDB1/xstream_tbs.dbf'
    SIZE 25M REUSE AUTOEXTEND ON MAXSIZE UNLIMITED;
  exit;

sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
  CREATE USER c##dbzuser IDENTIFIED BY dbz
    DEFAULT TABLESPACE xstream_tbs
    QUOTA UNLIMITED ON xstream_tbs
    CONTAINER=ALL;

  GRANT CREATE SESSION TO c##dbzuser CONTAINER=ALL;
  GRANT SET CONTAINER TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT ON V_$DATABASE to c##dbzuser CONTAINER=ALL;
  GRANT FLASHBACK ANY TABLE TO c##dbzuser CONTAINER=ALL;
  GRANT SELECT_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;
  GRANT EXECUTE_CATALOG_ROLE TO c##dbzuser CONTAINER=ALL;
  exit;
----

=== Create an XStream Outbound Server

Create an https://docs.oracle.com/cd/E11882_01/server.112/e16545/xstrm_cncpt.htm#XSTRM1088[XStream Outbound server]
(given the right privileges, this may be done automatically by the connector going forward, see {jira-url}/browse/DBZ-721[DBZ-721]):

.Create an XStream Outbound Server
[source,indent=0]
----
sqlplus c##dbzadmin/dbz@//localhost:1521/ORCLCDB
DECLARE
  tables  DBMS_UTILITY.UNCL_ARRAY;
  schemas DBMS_UTILITY.UNCL_ARRAY;
BEGIN
    tables(1)  := NULL;
    schemas(1) := 'debezium';
  DBMS_XSTREAM_ADM.CREATE_OUTBOUND(
    server_name     =>  'dbzxout',
    table_names     =>  tables,
    schema_names    =>  schemas);
END;
/
exit;
----

.Configure the XStream user account to connect to the XStream Outbound Server
[source,indent=0]
----
sqlplus sys/top_secret@//localhost:1521/ORCLCDB as sysdba
BEGIN
  DBMS_XSTREAM_ADM.ALTER_OUTBOUND(
    server_name  => 'dbzxout',
    connect_user => 'c##dbzuser');
END;
/
exit;
----

[NOTE]
====
A single XStream Outbound server cannot be shared by multiple {prodname} Oracle connectors.
Each connector requires a unique XStream Outbound connector to be configured.
====

[[selecting-the-adapter]]
=== Configuring the XStream adapter

By default {prodname} uses Oracle LogMiner to ingest change events from Oracle.
In order to use Oracle XStreams, the connector configuration must be adjusted to enable this adapter.

The following example configuration illustrates that by adding the `database.connection.adapter` and `database.out.server.name`,
the connector can be toggled to use the XStream API implementation.

[source,json,indent=0]
----
{
    "name": "inventory-connector",
    "config": {
        "connector.class" : "io.debezium.connector.oracle.OracleConnector",
        "tasks.max" : "1",
        "database.server.name" : "server1",
        "database.hostname" : "<oracle ip>",
        "database.port" : "1521",
        "database.user" : "c##dbzuser",
        "database.password" : "dbz",
        "database.dbname" : "ORCLCDB",
        "database.pdb.name" : "ORCLPDB1",
        "database.history.kafka.bootstrap.servers" : "kafka:9092",
        "database.history.kafka.topic": "schema-changes.inventory",
        "database.connection.adapter": "xstream",
        "database.out.server.name" : "dbzxout"
    }
}
----

[[oracle-xstreams-connector-properties]]
=== Connector properties

The following configuration properties are _required_ when using XStreams unless a default value is available.

[cols="30%a,25%a,45%a"]
|===
|Property
|Default
|Description

|[[oracle-property-database-out-server-name]]<<oracle-property-database-out-server-name, `+database.out.server.name+`>>
|
|Name of the XStream outbound server configured in the database.

|===

[[oracle-when-things-go-wrong]]
== Behavior when things go wrong

{prodname} is a distributed system that captures all changes in multiple upstream databases; it never misses or loses an event.
When the system is operating normally or being managed carefully then {prodname} provides _exactly once_ delivery of every change event record.

If a fault does happen then the system does not lose any events.
However, while it is recovering from the fault, it might repeat some change events.
In these abnormal situations, {prodname}, like Kafka, provides _at least once_ delivery of change events.

The rest of this section describes how {prodname} handles various kinds of faults and problems.

[id="oracle-cannot-reference-overflow-table"]
=== ORA-25191 - Cannot reference overflow table of an index-organized table

Oracle may issue this error during the snapshot phase when encountering an index-organized table (IOT).
This error means that the connector has attempted to execute an operation that must be executed against the parent index-organized table that contains the specified overflow table.

To resolve this, the IOT name used in the SQL operation should be replaced with the parent index-organized table name.
To determine the parent index-organized table name, use the following SQL:

```
SELECT IOT_NAME
  FROM DBA_TABLES
 WHERE OWNER='<tablespace-owner>'
   AND TABLE_NAME='<iot-table-name-that-failed>'
```

The connector's `table.include.list` or `table.exclude.list` configuration options should then be adjusted to explicitly include or exclude the appropriate tables to avoid the connector from attempting to capture changes from the child index-organized table.
