ifdef::[community]
[NOTE]
====
This feature is currently in incubating state. The exact semantics, configuration options, and so forth is subject to change in future revisions, based on the feedback we receive.
Please let us know if you encounter any problems while using this extension.
====
endif::[community]

ifdef::[product]
[IMPORTANT]
====
The use of incremental snapshots is a Technology Preview feature.
Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete;
therefore, Red Hat does not recommend implementing any Technology Preview features in production environments.
This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process.
For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[product]
To provide flexibility in managing snapshots, {prodname} includes a supplementary snapshot mechanism, known as _incremental snapshotting_.
Incremental snapshots rely on the {prodname} mechanism for xref:{link-signalling}#sending-signals-to-a-debezium-connector[sending signals to a {prodname} connector].
ifdef::[community]
Incremental snapshots are based on the link:https://github.com/debezium/debezium-design-documents/blob/main/DDD-3.md[DDD-3] design document.
endif::[community]

In an incremental snapshot, instead of capturing the full state of a database all at once, as in an initial snapshot, {prodname} captures each table in phases, in a series of configurable chunks.
You can specify the tables that you want the snapshot to capture and the xref:{context}-property-incremental-snapshot-chunk-size[size of each chunk].
The default chunk size for incremental snapshots is 1 KB.
As the snapshot proceeds, {prodname} uses watermarks to track its progress, maintaining a record of each table row that it captures.

This phased approach to capturing data provides the following advantages over the standard initial snapshot process:

* You can run incremental snapshots in parallel with streamed data capture, instead of postponing streaming until the snapshot completes.
  The connector continues to capture near real-time events from the change log throughout the snapshot process, and neither operation blocks the other.
* If the progress of an incremental snapshot is interrupted, you can resume it without losing any data.
  After the process resumes, the snapshot begins at the point where it stopped, rather than recapturing the table from the beginning.
* You can run an incremental snapshot on demand at any time, and repeat the process as needed to adapt to database updates.
  For example, you might re-run a snapshot after you modify the connector configuration to add a table to its `table.include.list`.

.Incremental snapshot process
When you run an incremental snapshot, {prodname} splits each table into chunks based on the primary keyâ€™s total order the configured chunk size.
During an incremental snapshot, {prodname} captures records from a table one chunk at a time, and emits a `READ` event for each record that it captures.
Each `READ` event represents the value of a table record at the time that the snapshot begins to capture the containing table chunk.
As the snapshot is underway, other processes can continue to modify records in the table.
As further updates are committed to the transaction log, {prodname} emits the appropriate `INSERT`,`UPDATE` or `DELETE` events for each change.
As a result, the Kafka topic for the table comes to include a mixture of different event types.

[id="{context}-how-debezium-resolves-conflicts-when-it-captures-multiple-events-for-records-that-have-the-same-primary-key"]
.How {prodname} resolves conflicts among records with the same primary key
Because {prodname} continues to stream changes in parallel with the snapshot process, streamed events can be received out of sequence.
That is, the streaming process might emit an `UPDATE` or `DELETE` event that modifies a table record before the snapshot captures the chunk that contains the `READ` event for that record.
The value that is represented in the `READ` event record is thus superseded before it's ever recorded.
//To resolve conflicts in the values of events that are received through the snapshot and streaming processes,
//effectively maintain the chronology of events when `INSERT`,`UPDATE` or `DELETE` events are received out of sequence,
To ensure that an incremental snapshot event that arrives out of sequence is processed in the correct logical order, {prodname} employs a buffering scheme for resolving conflicts.
Only after it resolves any conflicts between snapshot events and streamed events that affect the same record does {prodname} emit an event record to Kafka.

.Snapshot window
To ensure that earlier versions of a record do not overwrite later versions, {prodname} employs a so-called "snapshot window" to demarcate the interval during which an incremental snapshot captures data for a specified table chunk.
Before the snapshot for a chunk begins, {prodname} sends any events that it reads from the transaction log directly downstream to the target Kafka topic.
But after the snapshot window opens, that is, from the time that the snapshot for a particular chunk begins, until it ends, {prodname} performs a de-duplication step to resolve conflicts among event entries.

The snapshot captures events for each table chunk as `READ` events and writes them to a memory buffer.
Next, it examines any events that are streamed from the transaction log during the snapshot window, to verify whether they pertain to the table on which the snapshot operation is running.
If the transaction log events belong to the table, {prodname} next checks the primary key values in the transaction log events, and compares them to primary keys among the `READ` events in the buffer.
If it detects a match, {prodname} removes from matching `READ` event from the buffer, because, within the capture window, events received from the transaction log logically supersede `READ` events for the same primary key.
The transaction log event is then emitted to the topic for the table.
After the snapshot window for the chunk completes, only `READ` events for which no related transaction log events exist remain in the buffer.
{prodname} emits these remaining `READ` events to the table's Kafka topic.

The connector repeats the process for each snapshot chunk.

[id="debezium-{context}-triggering-an-incremental-snapshot"]
.Triggering an incremental snapshot

Currently, the only way to perform an incremental snapshot, is to initiate it as an {link-prefix}:#{context}-ad-hoc-snapshot[ad hoc snapshot].
You trigger an incremental snapshot by sending an ad hoc snapshot signal to the signaling table on the source database.
You send the signal by submitting it in a SQL `INSERT` query to the table.
After {prodname} detects the change in the signaling table, it reads the signal, and runs the requested snapshot operation.

The query that you submit specifies the kind of snapshot operation that you want to run the and the tables to include in the snapshot.
Currently, for snapshots operations, the only valid option is the default value, `incremental`.
If you do not specify a value, the connector runs an incremental snapshot.

To specify the tables to include in the snapshot, you create a `data-collections` array that lists the tables, for example, +
`{"data-collections": ["public.MyFirstTable", "public.MySecondTable"]}` +

The `data-collections` array for an incremental snapshot signal has no default value.
If the `data-collections` array  is empty, {prodname} detects that no action is required and does not perform a snapshot.

.Prerequisites

* xref:{link-signalling}#debezium-enabling-signaling"[Signaling is enabled]. +
** A signaling data collection exists on the source database and the connector is configured to capture it.
** The signaling data collection is specified in the xref:{context}-property-signal-data-collection[`signal.data.collection`] property.

.Procedure

. Send a SQL query to add the ad hoc incremental snapshot request to the signaling table:
+
[source,sql,indent=0,subs="+attributes"]
----
INSERT INTO _<signalTable>_ (id, type, data) VALUES (_'<id>'_, _'<snapshotType>'_, '{"data-collections": ["_<tableName>_","_<tableName>_"],"type":"_<snapshotType>_"}');
----
+
For example,
+
[source,sql,indent=0,subs="+attributes"]
----
INSERT INTO myschema.debezium_signal (id, type, data) VALUES('ad-hoc-1', 'execute-snapshot', '{"data-collections": ["schema1.table1", "schema1.table2"],"type":"incremental"}');
----
The values of the `id`,`type`, and `data` parameters in the command correspond to the xref:debezium-signaling-required-structure-of-a-signaling-data-collection[fields of the signaling table].
+
The following table describes the these parameters:
+
.Descriptions of fields in a SQL command for sending an incremental snapshot signal to the signaling table
[cols="1,4",options="header"]
|===
|Value |Description

|`myschema.debezium_signal`
|Specifies the fully-qualified name of the signaling table on the source database

|`ad-hoc-1`
| The `id` parameter specifies an arbitrary string that is assigned as the `id` identifier for the signal request. +
Use this string to identify logging messages to entries in the signaling table.
{prodname} does not use this string.
Rather, during the snapshot, {prodname} generates its own `id` string as a watermarking signal.

|`execute-snapshot`
| Specifies `type` parameter specifies the operation that the signal is intended to trigger. +

|`data-collections`
|A required component of the `data` field of a signal that specifies an array of table names to include in the snapshot. +
The array lists tables by their fully-qualified names, using the same format as you use to specify the name of the connector's signaling table in the xref:{context}-property-signal-data-collection[`signal.data.collection`] configuration property.

|`incremental`
|An optional `type` component of the `data` field of a signal that specifies the kind of snapshot operation to run. +
Currently, the only valid option is the default value, `incremental`. +
Specifying a `type` value in the SQL query that you submit to the signaling table is optional. +
If you do not specify a value, the connector runs an incremental snapshot.
|===

The following example, shows the JSON for an incremental snapshot event that is captured by a connector.

.Example: Incremental snapshot event message
[source,json,index=0]
----
{
    "before":null,
    "after": {
        "pk":"1",
        "value":"New data"
    },
    "source": {
        ...
        "snapshot":"incremental" <1>
    },
    "op":"r", <2>
    "ts_ms":"1620393591654",
    "transaction":null
}
----
[cols="1,1,4",options="header"]
|===
|Item |Field name |Description
|1
|`snapshot`
|Specifies the type of snapshot operation to run. +
Currently, the only valid option is the default value, `incremental`. +
Specifying a `type` value in the SQL query that you submit to the signaling table is optional. +
If you do not specify a value, the connector runs an incremental snapshot.

|2
|`op`
|Specifies the event type. +
The value for snapshot events is `r`, signifying a `READ` operation.

|===
