ifdef::[community]
[NOTE]
====
This feature is currently in incubating state. The exact semantics, configuration options, and so forth is subject to change in future revisions, based on the feedback we receive.
Please let us know if you encounter any problems while using this extension.
====
endif::[community]

ifdef::[product]
[IMPORTANT]
====
The use of incremental snapshots is a Technology Preview feature.
Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete;
therefore, Red Hat does not recommend implementing any Technology Preview features in production environments.
This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process.
For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[product]
To provide flexibility in managing snapshots, {prodname} includes a supplementary snapshot mechanism, known as _incremental snapshotting_.
Incremental snapshots enable you to capture a snapshot of a set of tables for a running connector.
Unlike an initial snapshot, which must capture the full state of a database all at once, during an incremental snapshot {prodname} captures data in a phases, in a series of configurable chunks.
The default chunk size for incremental snapshots is 1 KB.
You can configure the chunk size  xref:{context}-property-incremental-snapshot-chunk-size[`incremental.snapshot.chunk.size`].

Watermarks track the progress of the capture process, allowing you to pause and resume snapshots without losing data.
If the the snapshot is interrupted, it refers to the watermarks to determine which tables have been snapshot and which have not.
In this way, the snapshot can resume from the point at which it stops, rather than repeating the capture process from the beginning as happens after an initial snapshot is stopped. 

This phased approach to capturing data allows for the snapshot to run in parallel with streamed data capture.
Neither operation blocks the other.
Change data streaming runs continuously throughout the snapshot.
This contrasts with an initial snapshot, in which streaming is postponed until the snapshot completes. 
The ability to stream data during the snapshot, and to pause and resume snapshots on demand is especially useful when capturing databases that include large tables.
In a database that includes many large tables, an initial snapshot of a large data set might require hours, or even days, to complete.

Incremental snapshot provide a mechanism for capturing tables that are created after you initialize the connector.
You can modify the `include` list in the connector configuration to specify new tables to capture, and then run an incremental snapshot. 
The snapshot captures the new tables and resynchronize data in the Kafka topics for these tables with the current state of the database.

You can trigger incremental snapshots at any time and you can repeat them as needed.
Currently, the only way to perform an incremental snapshot, is to initiate it as an {link-prefix}:#{context}-ad-hoc-snapshot[ad hoc snapshot].

For more information about enabling signaling, see xref:{link-signalling}#sending-signals-to-a-debezium-connector[Sending signals to a {prodname} connector].



ifdef::[community]
Incremental snapshots are based on the link:https://github.com/debezium/debezium-design-documents/blob/main/DDD-3.md[DDD-3] design document.
endif::[community]

When you run an incremental snapshot, {prodname} continues to capture records related to ongoing table changes along with the snapshot records that it captures directly from source tables. 
During the window for capturing a table chunk, {prodname} compares the primary key of each table record in the snapshot to the keys  
of the `UPDATE` or `DELETE` event records that the connector obtains by streaming.
The state of an event that the connector captureds through the streaming process might be in conflict with the state represented by the snapshot process.
might be the incremental snapshot captures the table chunk that includes the "initial" state of that record,  tracks the event chronology so that earlier versions of a row do not overwrite later versions.
//In general, `READ` events represent the state of the record at an arbitrary point of time, rather than the initial state of the record in a table 
//{prodname} must maintain the chronology of the `READ` events that originate from the snapshot process alongside of any `UPDATE` or `DELETE` events that originate from the streaming process.

During an incremental snapshot, {prodname} reads the initial state of each table row directly from the source table and writes the data to Kafka as a `READ` event.
At the same time, the streaming process continues to capture change events that occur in each source table.
If a streamed event modifies an existing row, {prodname} captures the changes as `UPDATE` or `DELETE` events.
//Because of the way that the incremental snapshot captures tables in chunks, it's possible that an `UPDATE` or `DELETE` event that modifies a row might be written to the destination Kafka topic before the snapshot process captures the row.
Because of the way that the incremental snapshot captures tables in chunks, the connector might encounter `UPDATE` or `DELETE` events that modify a row before it captures the snapshot `READ` event that represents the row's initial state.
To maintain the chronology of events when `UPDATE` or `DELETE` events are received out of sequence, {prodname} first records the snapshot `READ` events for each chunk into a buffer.
It then performs a deduplication step to resolve conflicts among event entries.

For a specified chunk, if `UPDATE` or `DELETE` events exist for a corresponding `READ` event (that is, the events share a primary key), then {prodname} applies the following rules to disposition the `READ` event:

* If a `DELETE` event arrives before a corresponding `READ` event, the connector discards the `READ` event from the buffer.
* If an `UPDATE` event arrives before a corresponding `READ` event, the connector either discards the `READ` event or delivers it with the updated value.

The connector repeats the process for each snapshot chunk.

.Triggering an incremental snapshot

You trigger an incremental snapshot by sending a snapshot signal (`execute-snapshot`) to a designated signaling table.
You send the signal by submitting it in a SQL `INSERT` query to the table.
After {prodname} detects the change to the table, it captures the change event, which triggers it to initiate the requested operation.
//When {prodname} receives the `execute-snapshot` signal, it runs a snapshot operation to capture data from the specified tables.

.Prerequisites

* xref:{link-signalling}#debezium-enabling-signaling"[Signaling is enabled]. +

////
See the following properties for more information:
** xref:{context}-property-signal-data-collection[`signal.data.collection`]
** xref:{context}-property-incremental-snapshot-chunk-size[incremental.snapshot.chunk.size]
////

.Procedure

Compose a SQL query to send an ad hoc snapshot request to the signaling database.
//The query specifies the kind of operation that you want the connector to run. +
//Currently, for snapshots operations, the only valid option is the default value, `incremental`. +
//If you do not specify a value, the connector runs an incremental snapshot.


.Example of sending a SQL command to initiate an incremental snapshot
[source,sql,indent=0,subs="+attributes"]
----
INSERT INTO myschema.debezium_signal (id,`type`,`data`) VALUES('ad-hoc-1', 'execute-snapshot', '{"data-collections": ["schema1.table1", "schema1.table2"],"type":"INCREMENTAL"}');
----

|==

[cols="1,9",options="header"]
|===
|Column | Value

|id
|`d139b9b7-7777-4547-917d-e1775ea61d41`

|type
|`execute-snapshot`

|data
|`{"data-collections": ["public.MyFirstTable", "public.MySecondTable"]}`

|===


The values of the `id`,`type`, and `data` fields in the command correspond to the xref:debezium-signaling-required-structure-of-a-signaling-data-collection[fields of the signaling table].


For example,

[source,sql,indent=0,subs="+attributes"]
----
INSERT INTO _<schema>_.dbz_signal VALUES ('signal-1', 'execute-snapshot', '{\"data-collections\": [\"inventory.orders\"],"type":"INCREMENTAL"}');
----

The following example, shows the JSON for an incremental snapshot event that is captured by a connector.

.Example: Incremental snapshot event message
[source,json,index=0]
----
{
    "before":null,
    "after": {
        "pk":"1",
        "value":"New data"
    },
    "source": {
        ...
        "snapshot":"incremental" <1>
    },
    "op":"r", <2>
    "ts_ms":"1620393591654",
    "transaction":null
}
----
[cols="1,1,4",options="header"]
|===
|Item |Field name |Description
|1
|`snapshot`
|Specifies the type of snapshot operation to run. +
Currently, the only valid option is the default value, `incremental`. +
Specifying a `type` value in the SQL query that you submit to the signaling table is optional. +
If you do not specify a value, the connector runs an incremental snapshot.

|2
|`op`
|Specifies the event type. +
The value for snapshot events is `r`, signifying a `READ` operation.

|===

////
.Descriptions of fields in a SQL command for sending an incremental snapshot signal to the signaling table
[cols="1,4",options="header"]
|===
|Field |Description

|`id`
| An arbitrary string that that you assign to identify a signal request. +
WHen {prodname} runs the requested snapshot, it generates its own `id` string as a watermarking signal.

|`type`
| Specifies the kind of snapshot operation that you want the connector to run. +
Currently, the only valid option is the default value, `incremental`. +
Specifying a `type` value is optional. +
If you do not specify a value, the connector runs an incremental snapshot.
////
