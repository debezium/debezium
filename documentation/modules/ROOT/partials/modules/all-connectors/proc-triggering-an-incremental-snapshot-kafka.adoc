You can send a message to the {link-prefix}:{link-signalling}#debezium-signaling-enabling-kafka-signaling-channel[configured Kafka topic] to request the connector to run an ad hoc incremental snapshot.

The key of the Kafka message must match the value of the `topic.prefix` connector configuration option.

The value of the message is a JSON object with `type` and `data` fields.

The signal type is `execute-snapshot`, and the `data` field must have the following fields:

.Execute snapshot data fields
[cols="2,2,6a",options="header"]
|===
|Field | Default | Value

|`type`
|`incremental`
| The type of the snapshot to be executed.
Currently {prodname} supports the `incremental` and `blocking` types. +
See the next section for more details.

|`data-collections`
|_N/A_
| An array of comma-separated regular expressions that match the fully-qualified names of tables to include in the snapshot. +
Specify the names by using the same format as is required for the xref:{context}-property-signal-data-collection[signal.data.collection] configuration option.

|`additional-conditions`
|_N/A_
| An optional array of additional conditions that specifies criteria that the connector evaluates to designate a subset of records to include in a snapshot. +
Each additional condition is an object that specifies the criteria for filtering the data that an ad hoc snapshot captures.
You can set the following parameters for each additional condition:
`data-collection`:: The fully-qualified name of the {data-collection} that the filter applies to.
You can apply different filters to each {data-collection}.
`filter`:: Specifies column values that must be present in a database record for the snapshot to include it, for example,  `"color='blue'"`. +
 +
The values that you assign to the `filter` parameter are the same types of values that you might specify in the `WHERE` clause of `SELECT` statements when you set the `snapshot.select.statement.overrides` property for a blocking snapshot.
|===

.An `execute-snapshot` Kafka message
====
----
Key = `test_connector`

Value = `{"type":"execute-snapshot","data": {"data-collections": ["{collection-container}.table1", "{collection-container}.table2"], "type": "INCREMENTAL"}}`
----
====

.Ad hoc incremental snapshots with additional-conditions

{prodname} uses the `additional-conditions` field to select a subset of a {data-collection}'s content.

Typically, when {prodname} runs a snapshot, it runs a SQL query such as:

`SELECT * FROM _<tableName>_ ....`

When the snapshot request includes an `additional-conditions` property, the `data-collection` and `filter` parameters of the property are appended to the SQL query, for example:

`SELECT * FROM _<data-collection>_ WHERE _<filter>_ ....`

For example, given a `products` {data-collection} with the columns `id` (primary key), `color`, and `brand`, if you want a snapshot to include only content for which `color='blue'`, when you request the snapshot, you could add the `additional-conditions` property to filter the content:

include::{partialsdir}/modules/snippets/{context}-frag-signaling-fq-table-formats.adoc[leveloffset=+1,tags=triggering-incremental-snapshot-kafka-addtl-cond-example]

You can also use the `additional-conditions` property to pass conditions based on multiple columns.
For example, using the same `products` {data-collection} as in the previous example, if you want a snapshot to include only the content from the `products` {data-collection} for which `color='blue'`, and `brand='MyBrand'`, you could send the following request:

include::{partialsdir}/modules/snippets/{context}-frag-signaling-fq-table-formats.adoc[leveloffset=+1,tags=triggering-incremental-snapshot-kafka-multi-addtl-cond-example]
