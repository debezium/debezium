{prodname} provides a set of `schema.history.internal.*` properties that control how the connector interacts with the schema history topic.

The following table describes the `schema.history.internal` properties for configuring the {prodname} connector.

.Connector database schema history configuration properties
[cols="33%a,17%a,50%a",options="header",subs="+attributes"]
|===
|Property |Default |Description
|[[{context}-property-database-history-kafka-topic]]<<{context}-property-database-history-kafka-topic, `+schema.history.internal.kafka.topic+`>>
|No default
|The full name of the Kafka topic where the connector stores the database schema history.

|[[{context}-property-database-history-kafka-bootstrap-servers]]<<{context}-property-database-history-kafka-bootstrap-servers, `+schema.history.internal.kafka.bootstrap.servers+`>>
|No default
|A list of host/port pairs that the connector uses for establishing an initial connection to the Kafka cluster. This connection is used for retrieving the database schema history previously stored by the connector, and for writing each DDL statement read from the source database. Each pair should point to the same Kafka cluster used by the Kafka Connect process.

|[[{context}-property-database-history-kafka-recovery-poll-interval-ms]]<<{context}-property-database-history-kafka-recovery-poll-interval-ms, `+schema.history.internal.kafka.recovery.poll.interval.ms+`>>
|`100`
|An integer value that specifies the maximum number of milliseconds the connector should wait during startup/recovery while polling for persisted data. The default is 100ms.

|[[{context}-property-database-history-kafka-query-timeout-ms]]<<{context}-property-database-history-kafka-query-timeout-ms, `+schema.history.internal.kafka.query.timeout.ms+`>>
|`3000`
|An integer value that specifies the maximum number of milliseconds the connector should wait while fetching cluster information using Kafka admin client.

|[[{context}-property-database-history-kafka-create-timeout-ms]]<<{context}-property-database-history-kafka-create-timeout-ms, `+schema.history.internal.kafka.create.timeout.ms+`>>
|`30000`
|An integer value that specifies the maximum number of milliseconds the connector should wait while create kafka history topic using Kafka admin client.

|[[{context}-property-database-history-kafka-recovery-attempts]]<<{context}-property-database-history-kafka-recovery-attempts, `+schema.history.internal.kafka.recovery.attempts+`>>
|`100`
|The maximum number of times that the connector should try to read persisted history data before the connector recovery fails with an error. The maximum amount of time to wait after receiving no data is `recovery.attempts` Ã— `recovery.poll.interval.ms`.

|[[{context}-property-database-history-skip-unparseable-ddl]]<<{context}-property-database-history-skip-unparseable-ddl, `+schema.history.internal.skip.unparseable.ddl+`>>
|`false`
|A Boolean value that specifies whether the connector should ignore malformed or unknown database statements or stop processing so a human can fix the issue.
The safe default is `false`.
Skipping should be used only with care as it can lead to data loss or mangling when the binlog is being processed.

|[[{context}-property-database-history-store-only-captured-tables-ddl]]<<{context}-property-database-history-store-only-captured-tables-ddl, `+schema.history.internal.store.only.captured.tables.ddl+`>>
|`false`
|A Boolean value that specifies whether the connector records schema structures from all tables in a schema or database, or only from tables that are designated for capture. +
Specify one of the following values:

`false` (default):: During a database snapshot, the connector records the schema data for all non-system tables in the database, including tables that are not designated for capture.
It's best to retain the default setting.
If you later decide to capture changes from tables that you did not originally designate for capture, the connector can easily begin to capture data from those tables, because their schema structure is already stored in the schema history topic.
{prodname} requires the schema history of a table so that it can identify the structure that was present at the time that a change event occurred.

`true`:: During a database snapshot, the connector records the table schemas only for the tables from which {prodname} captures change events.
If you change the default value, and you later configure the connector to capture data from other tables in the database, the connector lacks the schema information that it requires to capture change events from the tables. +

|[[{context}-property-database-history-store-only-captured-databases-ddl]]<<{context}-property-database-history-store-only-captured-databases-ddl, `+schema.history.internal.store.only.captured.databases.ddl+`>>
|`false`
|A Boolean value that specifies whether the connector records schema structures from all logical databases in the database instance. +
Specify one of the following values:

`true`:: The connector records schema structures only for tables in the logical database and schema from which {prodname} captures change events.
`false`:: The connector records schema structures for all logical databases. +

NOTE: The default value is `true` for MySQL Connector +

|===

[id="{context}-pass-through-database-history-properties-for-configuring-producer-and-consumer-clients"]
.Pass-through database schema history properties for configuring producer and consumer clients
{empty} +
{prodname} relies on a Kafka producer to write schema changes to database schema history topics.
Similarly, it relies on a Kafka consumer to read from database schema history topics when a connector starts.
You define the configuration for the Kafka producer and consumer clients by assigning values to a set of pass-through configuration properties that begin with the `schema.history.internal.producer.\*` and `schema.history.internal.consumer.*` prefixes.
The pass-through producer and consumer database schema history properties control a range of behaviors, such as how these clients secure connections with the Kafka broker, as shown in the following example:

[source,indent=0]
----
schema.history.internal.producer.security.protocol=SSL
schema.history.internal.producer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
schema.history.internal.producer.ssl.keystore.password=test1234
schema.history.internal.producer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
schema.history.internal.producer.ssl.truststore.password=test1234
schema.history.internal.producer.ssl.key.password=test1234

schema.history.internal.consumer.security.protocol=SSL
schema.history.internal.consumer.ssl.keystore.location=/var/private/ssl/kafka.server.keystore.jks
schema.history.internal.consumer.ssl.keystore.password=test1234
schema.history.internal.consumer.ssl.truststore.location=/var/private/ssl/kafka.server.truststore.jks
schema.history.internal.consumer.ssl.truststore.password=test1234
schema.history.internal.consumer.ssl.key.password=test1234
----

{prodname} strips the prefix from the property name before it passes the property to the Kafka client.

See the Kafka documentation for more details about link:https://kafka.apache.org/documentation.html#producerconfigs[Kafka producer configuration properties] and link:https://kafka.apache.org/documentation.html#consumerconfigs[Kafka consumer configuration properties].
