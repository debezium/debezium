////

//Include in the deployment section that is conditionalized for [product]
// You can use either of the following methods to deploy a {prodname} connector:
//* Use {StreamsName} to automatically create a container image that includes the connector plug-in  This is the preferred method. [Links to Overview topic, which follows]
//* Build a custom Kafka Connect container image from a Dockerfile.

//.Additional resources
//[Links to connector configuration properties]

// Set the type to concept
//(// Type: concept
// Set the following ID
//[id="overview-of-using-streams-to-deploy-a-debezium-<context>-connector"]
The ID should explicitly specify the connector type vs. using the {context} variable
// Add the following heading in the connector file.
//=== Overview of using {StreamsName} to deploy a {prodname} {connector-name} connector
// Follow the title with the following INCLUDE statement
//include::{partialsdir}/modules/all-connectors/con-connector-streams-deployment.adoc[leveloffset=+1]

////

Beginning with {prodname} 1.6, the preferred method for deploying a {prodname} connector is to use {StreamsName} to build a Kafka Connect container image that includes the connector plug-in.

During the deployment process, you create and use the following custom resources (CRs):

* A `KafkaConnect` CR that defines your Kafka Connect instance and includes information about the connector artifacts needs to include in the image.
* A `KafkaConnector` CR that provides details that include information the connector uses to access the source database.
  After {kafka-streams} starts the Kafka Connect pod, you start the connector by applying the `KafkaConnector` CR.

In the build specification for the Kafka Connect image, you can specify the connectors that are available to deploy.
For each connector plug-in, you can also specify other components that you want to make available for deployment.
For example, you can add {registry} artifacts, or the {prodname} scripting component.
When {kafka-streams} builds the Kafka Connect image, it downloads the specified artifacts, and incorporates them into the image.

The `spec.build.output` parameter in the `KafkaConnect` CR specifies where to store the resulting Kafka Connect container image.
Container images can be stored in a Docker registry, or in an OpenShift ImageStream.
To store images in an ImageStream, you must create the ImageStream before you deploy Kafka Connect.
ImageStreams are not created automatically.


NOTE: If you use a `KafkaConnect` resource to create a cluster, afterwards you cannot use the Kafka Connect REST API to create or update connectors.
You can still use the REST API to retrieve information.

.Additional resources

* link:{LinkStreamsOpenShift}#proc-kafka-connect-config-str[Configuring Kafka Connect] in {NameStreamsOpenShift}.
* link:{LinkDeployStreamsOpenShift}#creating-new-image-using-kafka-connect-build-str[Creating a new container image automatically using {StreamsName} in {NameDeployStreamsOpenShift}].
