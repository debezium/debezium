When deploying {prodname} connectors on OpenShift, it's no longer necessary to first build your own Kafka Connect image.
Instead, the preferred method for deploying connectors on OpenShift is to use a build configuration in {kafka-streams} to automatically build a Kafka Connect container image that includes the {prodname} connector plug-ins that you want to use.

During the build process, the {kafka-streams} Operator transforms input parameters in a `KafkaConnect` custom resource, including {prodname} connector definitions, into a Kafka Connect container image.
The build downloads the necessary artifacts from the Red Hat Maven repository or another configured HTTP server.
The newly created container is pushed to the container registry that is specified in `.spec.build.output`, and is used to deploy a Kafka Connect pod.
After {StreamsName} builds the Kafka Connect image, you create `KafkaConnector` custom resources to start the connectors that included in the build.

.Prerequisites
* You have access to an OpenShift cluster on which the cluster Operator is installed.
* The {StreamsName} Operator is running.
* An Apache Kafka cluster is deployed as documented in link:{LinkDeployStreamsOpenShift}#kafka-cluster-str[{NameDeployStreamsOpenShift}].
* You have a {prodnamefull} license.
* The OpenShift `oc` CLI client is installed or you have access to the OpenShift Container Platform web console.
* Depending on how you intend to store the Kafka Connect build image, you need registry permissions or you must create an ImageStream resource:
+
To store the build image in an image registry, such as Red Hat Quay.io or Docker Hub::
** An account and permissions to create and manage images in the registry.

To store the build image as a native OpenShift ImageStream::
** An link:https://docs.openshift.com/container-platform/latest/openshift_images/images-understand.html#images-imagestream-use_images-understand[ImageStream] resource is deployed to the cluster.
You must explicitly create an ImageStream for the cluster.
ImageStreams are not available by default.

.Procedure

. Log in to the OpenShift cluster.
. Create a {prodname} `KafkaConnect` custom resource (CR) for the connector, or modify an existing one.
For example, create a `KafkaConnect` CR that specifies the `metadata.annotations` and `spec.build` properties, as shown in the following example.
Save the file with a name such as `dbz-connect.yaml`.
+
.A `dbz-inventory-connector.yaml` file that defines a `KafkaConnect` custom resource that includes a {prodname} connector
=====================================================================
[source,yaml,subs="+attributes,+quotes"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnect
metadata:
    name: debezium-kafka-connect-cluster
    annotations:
        strimzi.io/use-connector-resources: "true" // <1>
spec:
    version: 3.00
    build: // <2>
      output: // <3>
      type: imagestream  // <4>
      image: debezium-streams-connect:latest
      plugins: // <5>
      - name: debezium-connector-{connector-file}
        artifacts:
        - type: zip // <6>
          url: {red-hat-maven-repository}debezium/debezium-connector-{connector-file}/{debezium-version}-redhat-__<build_number>__/debezium-connector-{connector-file}-{debezium-version}.zip  // <7>
        - type: zip
          url: {red-hat-maven-repository}apicurio/apicurio-registry-distro-connect-converter/{registry-version}-redhat-_<build-number>_/apicurio-registry-distro-connect-converter-{registry-version}-redhat-_<build-number>_.zip
        - type: zip
          url: {red-hat-maven-repository}debezium/debezium-scripting/{debezium-version}/debezium-scripting-{debezium-version}.zip

  bootstrapServers: debezium-kafka-cluster-kafka-bootstrap:9093
----
.Descriptions of Kafka Connect configuration settings
[cols="1,7",options="header",subs="+attributes"]
|===
|Item |Description

|1
| Sets the `strimzi.io/use-connector-resources` annotation to `"true"` to enable the Cluster Operator to use `KafkaConnector` resources to configure connectors in this Kafka Connect cluster.

|2
|The `spec.build` configuration specifies where to store the build image and lists the plug-ins to include in the image, along with the location of the plug-in artifacts.

|3
|The `build.output` specifies the registry in which the newly built image is stored.

|4
|Specifies the name and image name for the image output.
Valid values for `output.type` are `docker` to push into a container registry like Docker Hub or Quay, or `imagestream` to push the image to an internal OpenShift ImageStream.
To use an ImageStream, an ImageStream resource must be deployed to the cluster.
For more information about specifying the `build.output` in the KafkaConnect configuration, see the link:{LinkStreamsOpenShift}#type-Build-reference[{StreamsName} Build schema reference documentation].

|5
|The `plugins` configuration lists all of the connectors that you want to include in the Kafka Connect image.
For each entry in the list, specify a plug-in `name`, and information for about the artifacts that are required to build the connector.
Optionally, for each connector plug-in, you can include other components that you want to be available for use with the connector.
For example, you can add Service Registry artifacts, or the {prodname} scripting component.

|6
|The value of `artifacts.type` specifies the file type of the artifact specified in the `artifacts.url`.
Valid types are `zip`, `tgz`, or `jar`.
{prodname} connector archives are provided in `.zip` file format.
JDBC driver files are in `.jar` format.
The `type` value must match the type of the file that is referenced in the `url` field.

|7
|The value of `artifacts.url` specifies the address of an HTTP server, such as a Maven repository, that stores the file for the connector artifact.
The OpenShift cluster must have access to the specified server.

|===
=====================================================================

. Apply the `KafkaConnect` build specification to the OpenShift cluster by entering the following command:
+
[source,shell,options="nowrap"]
----
oc create -f dbz-connect.yaml
----
+
Based on the configuration specified in the custom resource, the Streams Operator prepares a Kafka Connect image to deploy. +
After the build completes, the Operator pushes the image to the specified registry or ImageStream, and starts the Kafka Connect cluster.
The connector artifacts that you listed in the configuration are available in the cluster.

. Create a `KafkaConnector` resource to define an instance of each connector that you want to deploy. +
For example, create the following `KafkaConnector` CR, and save it as `{context}-inventory-connector.yaml`
+
.A `{context}-inventory-connector.yaml` file that defines the `KafkaConnector` custom resource for a {prodname} connector
=====================================================================

[source,yaml,subs="+attributes"]
----
apiVersion: {KafkaConnectApiVersion}
kind: KafkaConnector
metadata:
  labels:
    strimzi.io/cluster: debezium-kafka-connect-cluster
  name: inventory-connector-{context} // <1>
spec:
  class: io.debezium.connector.{context}.{connector-class}Connector // <2>
  tasksMax: 1  // <3>
  config:  // <4>
    database.history.kafka.bootstrap.servers: 'debezium-kafka-cluster-kafka-bootstrap.debezium.svc.cluster.local:9092'
    database.history.kafka.topic: schema-changes.inventory
    database.hostname: {context}.debezium-{context}.svc.cluster.local // <5>
    database.port: 3306   // <6>
    database.user: debezium  // <7>
    database.password: dbz  // <8>
    database.dbname: mydatabase // <9>
    database.server.name: inventory_connector_{context} // <10>
    database.include.list: public.inventory  // <11>
----

=====================================================================
+
.Descriptions of connector configuration settings
[cols="1,7",options="header",subs="+attributes"]
|===
|Item |Description

|1
|The name of the connector to register with the Kafka Connect cluster.

|2
|The name of the connector class.

|3
|The number of tasks that can operate concurrently.

|4
|The connectorâ€™s configuration.

|5
|The address of the host database instance.

|6
|The port number of the database instance.

|7
|The name of the user account through which {prodname} connects to the database.

|8
|The password for the database user account.

|9
|The name of the database to capture changes from.

|10
|The logical name of the database instance or cluster. +
The specified name must be formed only from alphanumeric characters or underscores. +
Because the logical name is used as the prefix for any Kafka topics that receive change events from this connector, the name must be unique among the connectors in the cluster. +
The namespace is also used in the names of related Kafka Connect schemas, and the namespaces of a corresponding Avro schema if you integrate the connector with the {link-prefix}:{link-avro-serialization}[Avro connector].

|11
|The list of tables from which the connector captures change events.

|===

. Create the connector resource by running the following command:
+
[source,shell,options="nowrap", subs="+attributes,+quotes"]
----
oc create -n __<namespace>__ -f __<kafkaConnector>__.yaml
----
+
For example,
+
[source,shell,options="nowrap"]
----
oc create -n debezium-docs -f dbz-connect.yaml
----
+
The connector is registered to the Kafka Connect cluster and starts to run against the database that is specified by `spec.config.database.dbname` in the `KafkaConnector` CR.
After the connector pod is ready, {prodname} is running.

You are now ready to xref:verifying-that-the-debezium-{context}-connector-is-running[verify the {prodname} {connector-name} deployment].
